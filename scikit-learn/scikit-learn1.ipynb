{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9386d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12abc2",
   "metadata": {},
   "source": [
    "---\n",
    "## Everything is **Estimators**(an object that learns from data)\n",
    "\n",
    "has a: \n",
    "- `fit()`\n",
    "---\t\n",
    "### **Transformer**: Estimator that change your Data\n",
    "\n",
    "has:\n",
    "- `fit()`\n",
    "- `transform()`\n",
    "- `fit_transform()`\n",
    "\n",
    "### Common **Transformers:**\n",
    "- **Scalers**:  Adjust Numeric range of features.\n",
    "    - `StandardScaler` ((Z-score) learns mean and std using `fit()` \n",
    "        - apply it on test data using `transform()`. \n",
    "        - Centers data around zero with unit variance.) **Centers at mean=0, std=1**\n",
    "        - > when to use:\n",
    "            - Most machine learning algorithms (logistic regression, SVM, KNN)\n",
    "            - When data has outliers (doesn't squeeze everything into fixed range)\n",
    "            - Generally more robust default choice\n",
    "    - `MiniMaxScaler` (Squashes everything b/w 0-1)\n",
    "        - > when to use:\n",
    "            - Neural networks (bounded activation functions like sigmoid)\n",
    "            - Image processing (pixel values 0-255 → 0-1)\n",
    "            - When you need specific range\n",
    "    - `RobustScaler`  (Uses median and IQR (interquartile range) Less affected by Outliers)\n",
    "    - `MaxAbsScaler`  (scales by max abs value)\n",
    "\n",
    "- **Encoders**: Transform Categories into numbers\n",
    "    - `OneHotEncoder` (Turns categories into binary columns)\n",
    "    - `OrdinalEncoder` (assigns integer codes to categories)\n",
    "\n",
    "- **Imputer**:  Fill in missing values\n",
    "    - `SimpleImputer` (Uses mean, median or mode)\n",
    "    - `KNNImputer` (uses nearby data points to estimate)\n",
    "    - `IterativeImputer` (Uses other feature to predict what's missing)\n",
    "- **Feature extractors**:\n",
    "    - `PolynomialFeature` (Generates interaction terms and polynomial combinations)\n",
    "    - `PCA` (Reduces dimentionality or to reduce complexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b451a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Imagine you have some features with different scales\n",
    "data = np.array([[1, 100],\n",
    "                 [2, 200],\n",
    "                 [3, 300],\n",
    "                 [4, 400]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit() examines the data and learns the mean and standard deviation\n",
    "scaler.fit(data)\n",
    "\n",
    "# After fitting, the scaler \"knows\" that column 1 has mean=2.5, std=1.12\n",
    "# and column 2 has mean=250, std=112\n",
    "print(f\"Learned means: {scaler.mean_}\")\n",
    "print(f\"Learned stds: {scaler.scale_}\")\n",
    "\n",
    "# transform() applies what it learned to actually scale the data\n",
    "scaled_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac57dd",
   "metadata": {},
   "source": [
    "---\n",
    "### **Predictors** : Estimators that make predctions. Also called classifiers/regressors\n",
    "\n",
    "has:\n",
    "- `fit()`\n",
    "- `predict()`\n",
    "- `preditct_proba()` : probability estimates\n",
    "- `score()` :  evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace28330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Your features and labels\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "# Create a predictor\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit() learns the relationship between X and y\n",
    "model.fit(X, y)\n",
    "\n",
    "# Now it can predict on new, unseen data\n",
    "new_data = [[2.5, 3.5]]\n",
    "prediction = model.predict(new_data)\n",
    "probabilities = model.predict_proba(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004980e4",
   "metadata": {},
   "source": [
    "---\n",
    "### **Pipelines**: Chaining Estimators Together\n",
    "**a sequence of transformers followed by a final predictor (or just transformers if you're only preprocessing).**\n",
    "\n",
    "- calling `fit()` on `Pipeline` calls `fit_transform()` on each `transformer` in sequence, then it calls `fit()` on `final Predictor`.\n",
    "- when you call `predict()` it calls `transform()` on each `transformer` then `predict()` on `final predictor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Step 1: Scale the features\n",
    "    ('pca', PCA(n_components=2)),      # Step 2: Reduce dimensionality\n",
    "    ('classifier', LogisticRegression()) # Step 3: Make predictions\n",
    "])\n",
    "\n",
    "# The beauty: you can treat the entire pipeline as a single estimator\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0beadde",
   "metadata": {},
   "source": [
    "---\n",
    "### **FeatureUnion and ColumnTransformer**: Parallel Processing\n",
    "to apply different transformations to different parts of your data simultaneously\n",
    "- **FeatureUnion** applies multiple transformers to the same data and concatenates the results horizontally. Imagine you want to extract both statistical features and text features from a dataset. FeatureUnion lets you do both transformations in parallel and combine them.\n",
    "- **ColumnTransformer** applies different transformers to different columns of your data. For datasets which have a mix of numeric and categorical features that need different preprocessing.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Suppose you have numeric columns [0, 1] and categorical columns [2, 3]\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numeric', StandardScaler(), [0, 1]),\n",
    "    ('categorical', OneHotEncoder(), [2, 3])\n",
    "])\n",
    "\n",
    "# This applies StandardScaler to the numeric columns\n",
    "# and OneHotEncoder to the categorical columns,\n",
    "# then concatenates the results\n",
    "preprocessed_data = preprocessor.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468514e",
   "metadata": {},
   "source": [
    "---\n",
    "### **Model Selection Tools:** Finding the Right Configuration\n",
    "- `Cross-validation` - divides your data into multiple folds and trains/tests on different combinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af083218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score=\n",
    "# This trains and evaluates your model 5 times with different data splits\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Average accuracy: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326c41d5",
   "metadata": {},
   "source": [
    "- **`GridSearchCV`** exhaustively tries every combination of hyperparameters you specify. \n",
    "    - It's like having a robot systematically test every possible configuration of your model to find the best one.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c48225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define which parameters to try\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# This tries all 6 combinations (3 C values × 2 penalties)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4bced",
   "metadata": {},
   "source": [
    "\n",
    ">***`Notice` how the parameter names use `double underscores`. This tells `GridSearchCV` to reach into the pipeline, find the step named `classifier`, and set its parameter `C` to these values. This works for arbitrarily nested structures.***\n",
    "\n",
    "- **`RandomizedSearchCV`** samples randomly instead of trying everything. Useful when you have too many hyperparameter combinations to test exhaustively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5808d",
   "metadata": {},
   "source": [
    "---\n",
    "### **Metrics and Scoring:** Measuring Success\n",
    "- For `classification`, \n",
    "    - `accuracy` is simple but can be misleading with imbalanced classes. \n",
    "    - `Precision` tells you what fraction of positive predictions were correct. \n",
    "    - `Recall` tells you what fraction of actual positives you found. \n",
    "    - `F1-score` balances precision and recall. \n",
    "    - `ROC-AUC` measures how well you separate classes across all decision thresholds. \n",
    "    - You choose based on your specific problem's `cost of false positives` versus `false negatives`.\n",
    "- For `regression`, \n",
    "    - `mean squared error` penalizes large errors heavily. \n",
    "    - `Mean absolute error` treats all errors equally. \n",
    "    - `R-squared` tells you what fraction of variance your model explains. \n",
    "    - You choose based on whether you care more about `avoiding large mistakes` or about `average performance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get detailed performance breakdown\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# See where your model makes mistakes\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110687b4",
   "metadata": {},
   "source": [
    "---\n",
    "### **Feature Selection**: Choosing What Matters\n",
    "Not all features are useful. `Feature selection tools` help identify and keep only the relevant ones.\n",
    "- `SelectKBest` chooses the top k features based on statistical tests. It's fast and simple but doesn't consider feature interactions.\n",
    "- `RFE (Recursive Feature Elimination)` repeatedly trains models and removes the weakest features. It's slower but considers how features work together.\n",
    "- `SelectFromModel` uses a trained model's feature importances to select features. This works well when your final model also provides importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e035901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select the 10 features with highest ANOVA F-values\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# You can use this in a pipeline too\n",
    "pipeline = Pipeline([\n",
    "    ('selector', SelectKBest(f_classif, k=10)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275211d",
   "metadata": {},
   "source": [
    "---\n",
    "### **Putting It All Together:** A Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fded13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Split your data first (never touch test data during development!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define preprocessing for different column types\n",
    "numeric_features = ['age', 'income', 'score']\n",
    "categorical_features = ['city', 'occupation']\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values\n",
    "    ('scaler', StandardScaler())                     # Scale to mean=0, std=1\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing categories\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))     # Convert to binary columns\n",
    "])\n",
    "\n",
    "# Combine preprocessing for different column types\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Create the full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),                    # Apply all preprocessing\n",
    "    ('feature_selection', SelectKBest(k=20)),         # Keep best features\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Train model\n",
    "])\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'feature_selection__k': [10, 20, 30],\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "# Find the best configuration\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_score}\")\n",
    "\n",
    "# The best model is ready to use on new data\n",
    "predictions = grid_search.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7d6b5",
   "metadata": {},
   "source": [
    "### **The Model Zoo:** What Algorithms Are Available\n",
    "- `Linear models` form the foundation. These include:\n",
    "    -  `LinearRegression` for predicting continuous values, \n",
    "    - `LogisticRegression` for binary and multiclass classification, \n",
    "    - `Ridge and Lasso` for regularized regression, and \n",
    "    - `ElasticNet` which combines both L1 and L2 penalties. \n",
    "    - There's also `SGDClassifier` and `SGDRegressor` which use stochastic gradient descent for large-scale learning. \n",
    "    - These models are fast, interpretable, and work well when your data has roughly linear relationships. They're often your first choice because they train quickly and you can understand exactly what they're doing.\n",
    "\n",
    "- `Tree-based` models build decision trees by recursively splitting your data.\n",
    "    - `DecisionTreeClassifier and DecisionTreeRegressor` are the basic versions, but they tend to overfit. \n",
    "    - That's why you'll more commonly use `ensemble` methods like `RandomForestClassifier` and `RandomForestRegressor`, which build many trees and average their predictions. \n",
    "    - `GradientBoostingClassifier` builds trees sequentially, where each new tree tries to correct the mistakes of previous ones. \n",
    "    - `There's also `HistGradientBoostingClassifier` which is faster for large datasets. \n",
    "    - Trees handle non-linear relationships naturally, work with mixed feature types without much preprocessing, and provide feature importance scores. The tradeoff is they're harder to interpret than linear models and can be computationally expensive.\n",
    "\n",
    "- `Support Vector Machines` find optimal boundaries between classes. \n",
    "    - `SVC` is for `classification` and `SVR` is for `regression`. They work by finding the `hyperplane` that maximizes the margin between classes. With different `kernels` (`linear`, `polynomial`, `RBF`), they can learn complex decision boundaries. \n",
    "    - `SVMs` work exceptionally well for `medium-sized datasets` and are particularly strong when you have `more features than samples`. However, they `scale poorly to very large datasets` and `require careful feature scaling.`\n",
    "\n",
    "- `Naive Bayes classifiers` use `probability theory`. \n",
    "    - `GaussianNB` assumes features follow a `normal distribution`, \n",
    "    - `MultinomialNB` works for count data like text, and \n",
    "    - `BernoulliNB` is for binary features. \n",
    "    - These are incredibly fast and work surprisingly `well for text classification` despite their \"naive\" assumption that features are independent. They `need very little training data` and `train in a single pass`, making them `perfect for streaming data` or `when you need quick baseline results.`\n",
    "\n",
    "- `Nearest neighbors methods` include \n",
    "    - `KNeighborsClassifier` and `KNeighborsRegressor`. These `don't really train` in the traditional sense `but memorize the training data and make predictions based on the closest examples`. \n",
    "    - They're intuitive, `naturally handle multi-class problems`, and `can learn very complex decision boundaries`. \n",
    "    - The `downside` is `prediction is slow` because it must `search` through `training data`, and they `struggle with high-dimensional data` due to the `curse of dimensionality`.\n",
    "\n",
    "- `Neural networks` in scikit-learn are represented by \n",
    "    - `MLPClassifier` and `MLPRegressor`, where MLP stands for `Multi-Layer Perceptron`. \n",
    "    - These `can learn highly complex patterns` but `require careful tuning of architecture and training parameters`. \n",
    "    - They're `useful for moderately complex patterns`, though for serious deep learning you'd use dedicated frameworks like `PyTorch` or `TensorFlow`.\n",
    "\n",
    "- `Ensemble methods` deserve special attention because they're `often your best performers`. \n",
    "    - The idea is simple yet powerful: combine multiple models to get better predictions than any single model. \n",
    "    - `VotingClassifier` and `VotingRegressor` combine different types of models through `voting` or `averaging`. \n",
    "    - `BaggingClassifier` and `BaggingRegressor` `train multiple instances of the same model on different subsets of your data`. \n",
    "    - `AdaBoostClassifier` and `AdaBoostRegressor` sequentially train models, `giving more weight to examples that previous models got wrong`. \n",
    "    - `StackingClassifier` trains a meta-model that `learns how to best combine predictions from multiple base models`. \n",
    "    - In practice, if you need the highest possible accuracy and can afford the computational cost, ensembles are where you'll often end up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626faf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb6794bf",
   "metadata": {},
   "source": [
    "### **Unsupervised Learning: Finding Structure Without Labels- Finding patterns in unlabeled data.**\n",
    "- `Clustering groups` similar items together. \n",
    "    - `KMeans` is the workhorse, partitioning data into k spherical clusters. It's fast and works well when clusters are roughly equal-sized and spherical. \n",
    "    - `DBSCAN` finds clusters of arbitrary shape and automatically identifies outliers, but you need to choose density parameters carefully. \n",
    "    - `AgglomerativeClustering` builds a hierarchy of clusters by progressively merging similar groups. \n",
    "    - `MeanShift` finds clusters by looking for density peaks without needing to specify the number of clusters upfront. \n",
    "    - `SpectralClustering` uses graph theory and works well for complex cluster shapes when you know the number of clusters.\n",
    "    - The key insight is that different clustering algorithms have different assumptions about what makes a `\"cluster.\"` \n",
    "        - `KMeans` assumes spherical shapes, \n",
    "        - `DBSCAN` assumes density-based connectivity, \n",
    "        - `hierarchical methods` assume you can build a tree of similarities. \n",
    "        - Choose based on your data's structure and what makes sense for your domain.\n",
    "\n",
    "- `Dimensionality reduction` compresses high-dimensional data while preserving important structure. \n",
    "    - `PCA` finds linear combinations of features that `capture maximum variance`. It's fast and works well when relationships are approximately linear. \n",
    "    - `TruncatedSVD` is similar but works with sparse matrices, making it `popular for text data`. \n",
    "    - `NMF` (Non-negative Matrix Factorization) constrains everything to be non-negative, which makes sense for `data like images or word counts` where you're modeling parts that add up. \n",
    "    - `TSNE` creates beautiful 2D or 3D visualizations by preserving local neighborhoods, though it's too slow for large datasets. \n",
    "    - `UMAP` is a newer alternative that's faster and often better at preserving global structure.\n",
    "\n",
    "- `Anomaly detection` identifies unusual examples. \n",
    "    - `IsolationForest` isolates anomalies by randomly splitting data, exploiting the fact that anomalies are rare and different. \n",
    "    - `LocalOutlierFactor` compares local density around each point to find outliers. \n",
    "    - `OneClassSVM` learns a boundary around normal data. \n",
    "    - `EllipticEnvelope` assumes your normal data follows a multivariate Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e805a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7aa8d05",
   "metadata": {},
   "source": [
    "### **Validation Strategies:** Beyond Basic Cross-Validation\n",
    "The way you validate your models dramatically affects how well you can trust their performance estimates. \n",
    "- `Basic k-fold` cross-validation is just the beginning.\n",
    "- `StratifiedKFold` ensures each fold has the `same class distribution as the full dataset`. \n",
    "    - This is `crucial for imbalanced classification` problems where random splitting might create folds with very few examples of minority classes. \n",
    "    - You should almost always use stratified splitting for classification unless you have a specific reason not to.\n",
    "- `GroupKFold` and related methods handle grouped data where you can't mix certain examples between train and test. \n",
    "    - Imagine you're predicting patient outcomes and you have multiple measurements per patient. If some measurements from a patient are in training and others in test, you're leaking information. \n",
    "    - GroupKFold keeps all examples from each patient together in either train or test, never split across both.\n",
    "- `TimeSeriesSplit` respects `temporal ordering`. \n",
    "    - Unlike regular cross-validation which randomly shuffles data, this always trains on past data and tests on future data. \n",
    "    - Each fold uses an expanding window of training data followed by a subsequent test period. \n",
    "    - This prevents the cardinal sin of time series analysis: training on the future to predict the past.\n",
    "- `RepeatedKFold` and `RepeatedStratifiedKFold` ***run k-fold cross-validation multiple times with different random seeds***. \n",
    "    - This gives you more robust estimates of model performance by reducing the variance that comes from a single random split.\n",
    "- `LeaveOneOut` and `LeavePOut**` use n-1 or n-p examples for training, testing on the remaining ones, and repeat for all combinations. \n",
    "    - These give nearly unbiased estimates but are computationally expensive and have high variance. \n",
    "    - They're mainly useful for small datasets where you can't afford to hold out much data.\n",
    "\n",
    "    >```python\n",
    "    >from sklearn.model_selection import (\n",
    "    >    StratifiedKFold, GroupKFold, TimeSeriesSplit,\n",
    "    >    cross_validate\n",
    "    >)\n",
    "    >\n",
    "    ># For imbalanced classification\n",
    "    >cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    >scores = cross_validate(model, X, y, cv=cv_stratified, \n",
    "    >                       scoring=['accuracy', 'f1', 'roc_auc'])\n",
    "    >\n",
    "    ># For grouped data (e.g., multiple samples per patient)\n",
    "    >cv_grouped = GroupKFold(n_splits=5)\n",
    "    >scores = cross_validate(model, X, y, cv=cv_grouped, groups=patient_ids)\n",
    "    >\n",
    "    ># For time series\n",
    "    >cv_time = TimeSeriesSplit(n_splits=5)\n",
    "    >scores = cross_validate(model, X, y, cv=cv_time)\n",
    "    >```\n",
    "\n",
    "> Notice the use of `cross_validate` instead of `cross_val_score`. This returns multiple metrics at once and also provides timing information, which helps you understand the computational cost of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d69c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13267677",
   "metadata": {},
   "source": [
    "### **Handling Imbalanced Data:** When Classes Aren't Equal\n",
    "- Real-world data is often imbalanced. You might have 95% negative examples and 5% positive ones. \n",
    "    - This creates problems because models can achieve high accuracy by simply predicting the majority class every time.\n",
    "\n",
    "- Scikit-learn provides several strategies. \n",
    "    - Many classifiers have a `class_weight` parameter that you can set to `'balanced'` to automatically weight classes inversely proportional to their frequency. \n",
    "    - This tells the model to pay more attention to minority class examples.\n",
    "\n",
    ">```python\n",
    "># The model will treat each class as equally important\n",
    ">model = RandomForestClassifier(class_weight='balanced')\n",
    ">```\n",
    "\n",
    "-You can also manually set class weights if you know the business cost of different errors. `Maybe a false negative costs you ten times more than a false positive`. You'd set weights accordingly.\n",
    "\n",
    "- Another approach is `resampling`. \n",
    "    - The `imblearn library` (which works seamlessly with scikit-learn) provides tools like \n",
    "        - `RandomOverSampler` to `duplicate minority examples`, \n",
    "        - `RandomUnderSampler` to `remove majority examples`, and \n",
    "        - `SMOTE` to `generate synthetic minority examples`. You can incorporate these directly into pipelines.\n",
    "\n",
    "- Perhaps most importantly, you need to `choose appropriate metrics`. \n",
    "    - `Accuracy` is meaningless for `imbalanced data.` Instead, \n",
    "    - look at `precision`, \n",
    "    - `recall`, \n",
    "    - `F1-score`, and particularly \n",
    "    - `ROC-AUC or PR-AUC` (precision-recall area under curve). \n",
    "    - The classification report gives you a comprehensive view.\n",
    "\n",
    ">```python\n",
    ">from sklearn.metrics import classification_report, roc_auc_score\n",
    ">\n",
    "># Get detailed breakdown by class\n",
    ">print(classification_report(y_true, y_pred))\n",
    ">\n",
    "># For binary classification with imbalance, PR-AUC is often better than ROC-AUC\n",
    ">from sklearn.metrics import average_precision_score\n",
    ">score = average_precision_score(y_true, y_pred_proba)\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10001b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01898f3a",
   "metadata": {},
   "source": [
    "### **Advanced Preprocessing:** The Full Toolkit\n",
    "Beyond what we covered earlier, scikit-learn has sophisticated `preprocessing` tools that handle edge cases and special situations.\n",
    "\n",
    "- **`Handling outliers`** matters because extreme values can distort your models. \n",
    "    - `RobustScaler` uses the interquartile range instead of mean and standard deviation, making it resistant to outliers. \n",
    "    - `QuantileTransformer` maps features to a uniform or normal distribution, which can help with heavily skewed data. \n",
    "    - `PowerTransformer` applies Box-Cox or Yeo-Johnson transformations to make data more Gaussian-like.\n",
    "\n",
    "- **`Binning and discretization`** convert continuous features into categorical ones. \n",
    "    - `KBinsDiscretizer` splits continuous features into intervals, which can help linear models capture non-linear relationships. \n",
    "        - Sometimes you know from domain expertise that certain ranges of a continuous variable should be treated categorically, and binning makes this explicit.\n",
    "\n",
    "- **`Feature engineering`** transformers create new features from existing ones. \n",
    "    - `PolynomialFeatures` generates interaction terms and polynomial combinations. \n",
    "        - For example, if you have features x1 and x2, it can create x1², x2², and x1×x2. \n",
    "    - `FunctionTransformer` lets you apply any custom function as part of a pipeline, giving you flexibility while maintaining the pipeline structure.\n",
    "\n",
    "- **`Text processing`** has its own specialized tools. \n",
    "    - `CountVectorizer` converts text documents into word count matrices. \n",
    "    - `TfidfVectorizer` does the same but weights words by their importance (rare words in a document but common across documents get higher weight). \n",
    "    - `HashingVectorizer` is memory-efficient for very large vocabularies. \n",
    "- These work seamlessly in pipelines alongside other preprocessing.\n",
    "\n",
    ">```python\n",
    ">from sklearn.feature_extraction.text import TfidfVectorizer\n",
    ">from sklearn.preprocessing import FunctionTransformer\n",
    ">import numpy as np\n",
    ">\n",
    "># Text pipeline\n",
    ">text_pipeline = Pipeline([\n",
    ">    ('tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
    ">    ('classifier', LogisticRegression())\n",
    ">])\n",
    ">\n",
    "># Custom transformation in a pipeline\n",
    ">log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    ">numeric_pipeline = Pipeline([\n",
    ">    ('log_transform', log_transformer),\n",
    ">    ('scaler', StandardScaler())\n",
    ">])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379340cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "312c903e",
   "metadata": {},
   "source": [
    "### **Model Inspection:** Understanding What Your Model Learned\n",
    "Building models is only half the battle. You need to understand what they've learned to trust them and explain them to others.\n",
    "\n",
    "- **`Feature importance`** tells you which features matter most. \n",
    "    - `Tree-based` models provide this naturally through their `feature_importances_ attribute`. \n",
    "    - For `linear models`, the `coefficients` tell you feature importance with direction (positive or negative effect). \n",
    "    - `Permutation importance` works for any model by shuffling each feature and measuring how much performance drops.\n",
    "    >```python\n",
    "    >from sklearn.inspection import permutation_importance\n",
    "    >\n",
    "    ># Works for any model\n",
    "    >result = permutation_importance(model, X_test, y_test, n_repeats=10)\n",
    "    >importance_df = pd.DataFrame({\n",
    "    >    'feature': feature_names,\n",
    "    >    'importance': result.importances_mean,\n",
    "    >    'std': result.importances_std\n",
    "    >}).sort_values('importance', ascending=False)\n",
    "\n",
    "- **`Partial dependence`** plots show how predictions change as you vary one or two features while averaging over all others. \n",
    "    - This reveals the marginal effect of features and can expose non-linear relationships.\n",
    "\n",
    "    >```python\n",
    "    >from sklearn.inspection import PartialDependenceDisplay\n",
    "    >\n",
    "    ># Show how predictions depend on two features\n",
    "    >features = ['age', 'income']\n",
    "    >PartialDependenceDisplay.from_estimator(model, X, features)\n",
    "- **`Learning curves`** plot `training` and `validation scores` as you increase training set size. \n",
    "    - They help diagnose whether you'd benefit from `more data (high bias)` or `simpler models (high variance)`.\n",
    "\n",
    "    >```python\n",
    "    >from sklearn.model_selection import learning_curve\n",
    "    >\n",
    "    >train_sizes, train_scores, val_scores = learning_curve(\n",
    "    >    model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "    >)\n",
    "    ># Plot these to see if more data would help\n",
    "- **`Validation`** curves show how performance changes as you vary a hyperparameter. \n",
    "    - This helps you understand if you're in the right range for your `hyperparameters` and whether you're `overfitting` or `underfitting`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820627f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04eaeb0e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### **Structuring Real ML Projects:** The Professional Approach\n",
    "- How to structure actual ML code so it's maintainable, reproducible, and professional.\n",
    "- Separate your concerns by organizing code into `logical` modules. \n",
    "    - Have a `data loading` module, \n",
    "    - a `feature engineering` module, \n",
    "    - a `model building` module, and \n",
    "    - an `evaluation` module. \n",
    "- Each should have a clear responsibility and clean interfaces between them.\n",
    "```python\n",
    "# project_structure/\n",
    "# ├── data/\n",
    "# │   ├── raw/\n",
    "# │   └── processed/\n",
    "# ├── notebooks/\n",
    "# │   └── exploration.ipynb\n",
    "# ├── src/\n",
    "# │   ├── __init__.py\n",
    "# │   ├── data.py          # Data loading and splitting\n",
    "# │   ├── features.py      # Feature engineering pipelines\n",
    "# │   ├── models.py        # Model definitions\n",
    "# │   ├── evaluate.py      # Evaluation functions\n",
    "# │   └── train.py         # Training scripts\n",
    "# ├── tests/\n",
    "# │   └── test_features.py\n",
    "# ├── models/\n",
    "# │   └── trained_models/\n",
    "# └── requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062654",
   "metadata": {},
   "source": [
    "- Your **`data module`** handles `loading` and `splitting` consistently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10846c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from CSV with consistent dtypes and validation.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Add validation checks\n",
    "    required_cols = ['feature1', 'feature2', 'target']\n",
    "    assert all(col in df.columns for col in required_cols)\n",
    "    return df\n",
    "\n",
    "def split_data(df, target_col, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"Create train/val/test splits consistently.\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # First split off test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Then split remaining into train and validation\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, \n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd515df6",
   "metadata": {},
   "source": [
    "- Your **`features module`** defines `reusable preprocessing pipelines`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/features.py\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def create_preprocessor(numeric_features, categorical_features):\n",
    "    \"\"\"Create preprocessing pipeline for the dataset.\"\"\"\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'  # Explicitly drop columns not specified\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19c5e4",
   "metadata": {},
   "source": [
    "- Your **`models`** module defines `model configurations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/models.py\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_model_config():\n",
    "    \"\"\"Return dictionary of models to try with their param grids.\"\"\"\n",
    "    return {\n",
    "        'logistic': {\n",
    "            'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'params': {\n",
    "                'classifier__C': [0.1, 1.0, 10.0],\n",
    "                'classifier__penalty': ['l2']\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            'params': {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__max_depth': [10, 20, None],\n",
    "                'classifier__min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__learning_rate': [0.01, 0.1],\n",
    "                'classifier__max_depth': [3, 5]\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2ae34",
   "metadata": {},
   "source": [
    "- Your **`evaluation`** module provides consistent metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/evaluate.py\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(model, X, y, dataset_name='test'):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'dataset': dataset_name,\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, average='binary'),\n",
    "        'recall': recall_score(y, y_pred, average='binary'),\n",
    "        'f1': f1_score(y, y_pred, average='binary')\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()} SET RESULTS:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    return pd.Series(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee08450",
   "metadata": {},
   "source": [
    "- Finally, your **`training`** script orchestrates everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a642f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/train.py\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from data import load_data, split_data\n",
    "from features import create_preprocessor\n",
    "from models import get_model_config\n",
    "from evaluate import evaluate_model\n",
    "\n",
    "def train_and_evaluate(data_path, model_name, output_dir='models'):\n",
    "    \"\"\"Complete training pipeline.\"\"\"\n",
    "    # Load and split data\n",
    "    df = load_data(data_path)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, 'target')\n",
    "    \n",
    "    # Define feature types (could also be loaded from config)\n",
    "    numeric_features = ['age', 'income', 'score']\n",
    "    categorical_features = ['city', 'occupation']\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessor(numeric_features, categorical_features)\n",
    "    \n",
    "    # Get model configuration\n",
    "    model_configs = get_model_config()\n",
    "    config = model_configs[model_name]\n",
    "    \n",
    "    # Create full pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', config['model'])\n",
    "    ])\n",
    "    \n",
    "    # Hyperparameter search with cross-validation on training set\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        config['params'],\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Training {model_name}...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics = evaluate_model(grid_search, X_val, y_val, 'validation')\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    test_metrics = evaluate_model(grid_search, X_test, y_test, 'test')\n",
    "    \n",
    "    # Save the model\n",
    "    output_path = Path(output_dir) / f'{model_name}_model.pkl'\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    joblib.dump(grid_search.best_estimator_, output_path)\n",
    "    print(f\"Model saved to {output_path}\")\n",
    "    \n",
    "    return grid_search, val_metrics, test_metrics\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    \n",
    "    data_path = sys.argv[1] if len(sys.argv) > 1 else 'data/processed/data.csv'\n",
    "    model_name = sys.argv[2] if len(sys.argv) > 2 else 'random_forest'\n",
    "    \n",
    "    train_and_evaluate(data_path, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007fdee",
   "metadata": {},
   "source": [
    "- This structure makes your work reproducible, testable, and collaborative. \n",
    "- Each module has a single responsibility, making debugging easier. \n",
    "- You can import these functions in notebooks for exploration or run them as scripts for production training.\n",
    "\n",
    "- `Configuration management` is crucial for reproducibility. \n",
    "    - Use config files (YAML or JSON) to store `hyperparameters`, `feature definitions`, and `data paths` rather than hardcoding them. \n",
    "    - This makes it easy to experiment with different configurations without changing code.\n",
    "\n",
    "- `Model versioning` matters when you're iterating. \n",
    "    - Save not just the `model` but also `metadata` about the `data version`, `features used`, `hyperparameters`, and `performance metrics`. \n",
    "    - This lets you reproduce any model you've trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_with_metadata(model, metadata, model_name):\n",
    "    \"\"\"Save model along with training metadata.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_path = f'models/{model_name}_{timestamp}.pkl'\n",
    "    metadata_path = f'models/{model_name}_{timestamp}_metadata.json'\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved model to {model_path}\")\n",
    "    print(f\"Saved metadata to {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b436b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **`Testing your pipelines`** prevents subtle bugs. \n",
    "    - Write unit tests for your feature engineering to ensure transformations work as expected. \n",
    "    - Test that your pipeline handles edge cases like missing values or unseen categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_features.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from src.features import create_preprocessor\n",
    "\n",
    "def test_preprocessor_handles_missing_values():\n",
    "    \"\"\"Test that preprocessor correctly imputes missing values.\"\"\"\n",
    "    X = pd.DataFrame({\n",
    "        'numeric_col': [1, None, 3],\n",
    "        'cat_col': ['A', 'B', None]\n",
    "    })\n",
    "    \n",
    "    preprocessor = create_preprocessor(['numeric_col'], ['cat_col'])\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # No NaN values should remain\n",
    "    assert not pd.isna(X_transformed).any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7c506",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dcb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88bda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c193b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c4caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17af3c1e",
   "metadata": {},
   "source": [
    "I'll help you build a complete mental model of scikit-learn from the ground up. Think of this as learning the \"grammar\" of the library before you start speaking the language.\n",
    "\n",
    "## The Core Philosophy: Everything is an Estimator\n",
    "\n",
    "Scikit-learn is built around one central idea: almost everything you work with is an **estimator**. An estimator is simply any object that learns from data. This might sound abstract, but it creates a beautiful consistency throughout the entire library.\n",
    "\n",
    "Every estimator follows the same pattern. It has a `fit()` method that learns something from your data. What it learns depends on what kind of estimator it is, but the interface is always the same. This consistency means once you understand the pattern, you can pick up any new algorithm or tool quickly.\n",
    "\n",
    "Let me show you how this plays out across different types of estimators.\n",
    "\n",
    "## Transformers: Estimators That Change Your Data\n",
    "\n",
    "A **transformer** is a special type of estimator that learns how to modify your data. It has both `fit()` and `transform()` methods, and usually a convenience method called `fit_transform()` that does both at once.\n",
    "\n",
    "Think of a transformer like a chef who needs to prep ingredients. The chef first examines your raw ingredients to figure out what prep work is needed (that's `fit()`), then actually performs that prep work (that's `transform()`).\n",
    "\n",
    "Here's a concrete example with a StandardScaler, which is one of the most common transformers:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Imagine you have some features with different scales\n",
    "data = np.array([[1, 100],\n",
    "                 [2, 200],\n",
    "                 [3, 300],\n",
    "                 [4, 400]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit() examines the data and learns the mean and standard deviation\n",
    "scaler.fit(data)\n",
    "\n",
    "# After fitting, the scaler \"knows\" that column 1 has mean=2.5, std=1.12\n",
    "# and column 2 has mean=250, std=112\n",
    "print(f\"Learned means: {scaler.mean_}\")\n",
    "print(f\"Learned stds: {scaler.scale_}\")\n",
    "\n",
    "# transform() applies what it learned to actually scale the data\n",
    "scaled_data = scaler.transform(data)\n",
    "```\n",
    "\n",
    "The key insight here is that `fit()` is where learning happens. The scaler looks at your training data and remembers the statistics. Then `transform()` applies those learned statistics, which is crucial because you'll use the exact same transformation on new data later without re-fitting.\n",
    "\n",
    "Common transformers you'll encounter include:\n",
    "\n",
    "**Scalers** are transformers that adjust the numeric range of your features. StandardScaler centers data around zero with unit variance. MinMaxScaler squashes everything between zero and one. RobustScaler is similar but less affected by outliers. MaxAbsScaler scales by the maximum absolute value. You use these when different features have wildly different scales (like age in years versus salary in dollars), which can confuse many machine learning algorithms.\n",
    "\n",
    "**Encoders** transform categorical data into numbers. OneHotEncoder turns categories into binary columns (if you have colors red, blue, green, it creates three separate 0/1 columns). OrdinalEncoder assigns integer codes to categories. LabelEncoder is specifically for target variables. You use these because most ML algorithms can only work with numbers, not text categories.\n",
    "\n",
    "**Imputers** fill in missing values. SimpleImputer can use strategies like mean, median, or most frequent value. KNNImputer uses nearby data points to estimate missing values. IterativeImputer uses other features to predict what's missing. You use these because most algorithms can't handle missing data and will simply crash or give errors.\n",
    "\n",
    "**Feature extractors** create new features from existing ones. PolynomialFeatures generates interaction terms and polynomial combinations. PCA reduces dimensionality while preserving variance. You use these to create richer representations or to reduce complexity.\n",
    "\n",
    "## Predictors: Estimators That Make Predictions\n",
    "\n",
    "A **predictor** (also called a model or classifier/regressor) is an estimator that learns patterns to make predictions. It has `fit()` and `predict()` methods. Some predictors also have `predict_proba()` for probability estimates or `score()` for evaluating accuracy.\n",
    "\n",
    "Think of a predictor like a student studying for an exam. During `fit()`, the student studies the training examples and their answers, learning the patterns. During `predict()`, the student takes the exam on new questions they haven't seen before.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Your features and labels\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "# Create a predictor\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit() learns the relationship between X and y\n",
    "model.fit(X, y)\n",
    "\n",
    "# Now it can predict on new, unseen data\n",
    "new_data = [[2.5, 3.5]]\n",
    "prediction = model.predict(new_data)\n",
    "probabilities = model.predict_proba(new_data)\n",
    "```\n",
    "\n",
    "The distinction between transformers and predictors is crucial: transformers change your input data into a different form, while predictors learn to map inputs to outputs (labels or values).\n",
    "\n",
    "## The Critical fit/transform Split\n",
    "\n",
    "Here's something that trips up many beginners: why do we have separate `fit()` and `transform()` methods instead of just one method that does everything?\n",
    "\n",
    "The answer is about preventing data leakage. Imagine you're scaling your data. You need to compute the mean and standard deviation from your training data, then apply that same scaling to your test data. If you fit on the test data, you're \"peeking\" at information you shouldn't have access to during training, which artificially inflates your performance metrics.\n",
    "\n",
    "```python\n",
    "# CORRECT WAY\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Learn statistics from training data only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply training statistics to test data\n",
    "\n",
    "# WRONG WAY - Don't do this!\n",
    "scaler_wrong = StandardScaler()\n",
    "X_train_scaled = scaler_wrong.fit_transform(X_train)\n",
    "X_test_scaled = scaler_wrong.fit_transform(X_test)  # This re-fits on test data!\n",
    "```\n",
    "\n",
    "This separation is fundamental to how scikit-learn prevents you from accidentally cheating.\n",
    "\n",
    "## Pipelines: Chaining Estimators Together\n",
    "\n",
    "Once you understand estimators, transformers, and predictors, **pipelines** become obvious. A pipeline is simply a sequence of transformers followed by a final predictor (or just transformers if you're only preprocessing).\n",
    "\n",
    "Think of a pipeline like an assembly line in a factory. Raw materials enter at one end, go through various processing stations, and emerge as a finished product at the other end. Each station knows its job and passes its output to the next station.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Step 1: Scale the features\n",
    "    ('pca', PCA(n_components=2)),      # Step 2: Reduce dimensionality\n",
    "    ('classifier', LogisticRegression()) # Step 3: Make predictions\n",
    "])\n",
    "\n",
    "# The beauty: you can treat the entire pipeline as a single estimator\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "When you call `fit()` on a pipeline, it does something clever. It calls `fit_transform()` on each transformer in sequence, passing the output of one as input to the next. Then it calls `fit()` on the final predictor. When you call `predict()`, it calls `transform()` on each transformer, then `predict()` on the final predictor.\n",
    "\n",
    "The power of pipelines is that they guarantee your preprocessing steps are applied consistently and correctly. You can't accidentally forget a step or apply them in the wrong order. They also make hyperparameter tuning much cleaner, as you'll see later.\n",
    "\n",
    "## FeatureUnion and ColumnTransformer: Parallel Processing\n",
    "\n",
    "Sometimes you need to apply different transformations to different parts of your data simultaneously. This is where **FeatureUnion** and **ColumnTransformer** come in.\n",
    "\n",
    "FeatureUnion applies multiple transformers to the same data and concatenates the results horizontally. Imagine you want to extract both statistical features and text features from a dataset. FeatureUnion lets you do both transformations in parallel and combine them.\n",
    "\n",
    "ColumnTransformer is more specific and often more useful. It applies different transformers to different columns of your data. This is perfect for real-world datasets where you have a mix of numeric and categorical features that need different preprocessing.\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Suppose you have numeric columns [0, 1] and categorical columns [2, 3]\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numeric', StandardScaler(), [0, 1]),\n",
    "    ('categorical', OneHotEncoder(), [2, 3])\n",
    "])\n",
    "\n",
    "# This applies StandardScaler to the numeric columns\n",
    "# and OneHotEncoder to the categorical columns,\n",
    "# then concatenates the results\n",
    "preprocessed_data = preprocessor.fit_transform(data)\n",
    "```\n",
    "\n",
    "You can think of ColumnTransformer as having multiple assembly lines running in parallel for different types of materials, then combining everything at the end.\n",
    "\n",
    "## Model Selection Tools: Finding the Right Configuration\n",
    "\n",
    "Scikit-learn provides powerful tools for finding the best model and hyperparameters. These tools work by treating your pipeline or model as a black box that they can configure and evaluate repeatedly.\n",
    "\n",
    "**Cross-validation** is the foundation. Instead of using a single train/test split, it divides your data into multiple folds and trains/tests on different combinations. This gives you a more robust estimate of how well your model will perform.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# This trains and evaluates your model 5 times with different data splits\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Average accuracy: {scores.mean()}\")\n",
    "```\n",
    "\n",
    "**GridSearchCV** exhaustively tries every combination of hyperparameters you specify. It's like having a robot systematically test every possible configuration of your model to find the best one.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define which parameters to try\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# This tries all 6 combinations (3 C values × 2 penalties)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "Notice how the parameter names use double underscores. This tells GridSearchCV to reach into the pipeline, find the step named 'classifier', and set its parameter 'C' to these values. This works for arbitrarily nested structures.\n",
    "\n",
    "**RandomizedSearchCV** is similar but samples randomly instead of trying everything. This is useful when you have too many hyperparameter combinations to test exhaustively.\n",
    "\n",
    "## Metrics and Scoring: Measuring Success\n",
    "\n",
    "Scikit-learn provides many ways to evaluate your models, and understanding when to use each is important.\n",
    "\n",
    "For classification, accuracy is simple but can be misleading with imbalanced classes. Precision tells you what fraction of positive predictions were correct. Recall tells you what fraction of actual positives you found. F1-score balances precision and recall. ROC-AUC measures how well you separate classes across all decision thresholds. You choose based on your specific problem's cost of false positives versus false negatives.\n",
    "\n",
    "For regression, mean squared error penalizes large errors heavily. Mean absolute error treats all errors equally. R-squared tells you what fraction of variance your model explains. You choose based on whether you care more about avoiding large mistakes or about average performance.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get detailed performance breakdown\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# See where your model makes mistakes\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "```\n",
    "\n",
    "## Feature Selection: Choosing What Matters\n",
    "\n",
    "Not all features are useful. **Feature selection** tools help you identify and keep only the relevant ones.\n",
    "\n",
    "SelectKBest chooses the top k features based on statistical tests. It's fast and simple but doesn't consider feature interactions.\n",
    "\n",
    "RFE (Recursive Feature Elimination) repeatedly trains models and removes the weakest features. It's slower but considers how features work together.\n",
    "\n",
    "SelectFromModel uses a trained model's feature importances to select features. This works well when your final model also provides importance scores.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select the 10 features with highest ANOVA F-values\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# You can use this in a pipeline too\n",
    "pipeline = Pipeline([\n",
    "    ('selector', SelectKBest(f_classif, k=10)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "```\n",
    "\n",
    "## Putting It All Together: A Complete Workflow\n",
    "\n",
    "Let me show you how all these pieces fit together in a realistic example:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Split your data first (never touch test data during development!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define preprocessing for different column types\n",
    "numeric_features = ['age', 'income', 'score']\n",
    "categorical_features = ['city', 'occupation']\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values\n",
    "    ('scaler', StandardScaler())                     # Scale to mean=0, std=1\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing categories\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))     # Convert to binary columns\n",
    "])\n",
    "\n",
    "# Combine preprocessing for different column types\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Create the full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),                    # Apply all preprocessing\n",
    "    ('feature_selection', SelectKBest(k=20)),         # Keep best features\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Train model\n",
    "])\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'feature_selection__k': [10, 20, 30],\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "# Find the best configuration\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_score}\")\n",
    "\n",
    "# The best model is ready to use on new data\n",
    "predictions = grid_search.predict(new_data)\n",
    "```\n",
    "\n",
    "This example shows the complete pattern: preprocess different column types appropriately, select important features, tune hyperparameters with cross-validation, and evaluate on held-out test data. This is the structure you'll use repeatedly.\n",
    "\n",
    "## Common Gotchas and Mental Traps\n",
    "\n",
    "Now that you understand the architecture, let me warn you about common mistakes. Many beginners fit transformers on the entire dataset before splitting into train/test sets. This leaks information. Always split first, then fit only on training data.\n",
    "\n",
    "Another trap is forgetting that some transformers need the target variable during fit. For example, feature selection methods need y to determine which features are relevant. Don't be surprised when you see `selector.fit(X, y)` instead of just `selector.fit(X)`.\n",
    "\n",
    "Some methods like PCA or certain scalers can change the number or meaning of your features. After PCA, you no longer have your original features but rather principal components. This matters for interpretation.\n",
    "\n",
    "Finally, not all estimators can be used in all contexts. Classifiers expect discrete categories as targets, while regressors expect continuous values. Some estimators require dense arrays while others work with sparse matrices. The documentation will tell you, but understanding these requirements prevents confusion.\n",
    "\n",
    "## The Ecosystem Beyond Basics\n",
    "\n",
    "As you grow more comfortable, you'll discover that scikit-learn has tools for almost every common ML task. There are ensemble methods that combine multiple models. There are clustering algorithms for unsupervised learning. There are decomposition techniques beyond PCA. There are specialized tools for text data, time series, and graph structures.\n",
    "\n",
    "The beautiful thing is that once you understand the estimator pattern, fit/transform/predict interface, and pipeline philosophy, you can pick up any new tool in the library quickly. They all follow the same design principles.\n",
    "\n",
    "Would you like me to dive deeper into any particular area, or would you prefer to see some hands-on examples working through a specific machine learning problem from start to finish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da805d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0236816",
   "metadata": {},
   "source": [
    "I'll take you deeper into the landscape, showing you both what's available and how experienced practitioners structure their ML work. Think of this as moving from understanding the individual tools to understanding how to orchestrate them into a coherent system.\n",
    "\n",
    "## The Model Zoo: What Algorithms Are Available\n",
    "\n",
    "Scikit-learn organizes its algorithms into clear families, and understanding this taxonomy helps you know where to look when you need something specific.\n",
    "\n",
    "**Linear models** form the foundation. These include LinearRegression for predicting continuous values, LogisticRegression for binary and multiclass classification, Ridge and Lasso for regularized regression, and ElasticNet which combines both L1 and L2 penalties. There's also SGDClassifier and SGDRegressor which use stochastic gradient descent for large-scale learning. These models are fast, interpretable, and work well when your data has roughly linear relationships. They're often your first choice because they train quickly and you can understand exactly what they're doing.\n",
    "\n",
    "**Tree-based models** build decision trees by recursively splitting your data. DecisionTreeClassifier and DecisionTreeRegressor are the basic versions, but they tend to overfit. That's why you'll more commonly use ensemble methods like RandomForestClassifier and RandomForestRegressor, which build many trees and average their predictions. GradientBoostingClassifier builds trees sequentially, where each new tree tries to correct the mistakes of previous ones. There's also HistGradientBoostingClassifier which is faster for large datasets. Trees handle non-linear relationships naturally, work with mixed feature types without much preprocessing, and provide feature importance scores. The tradeoff is they're harder to interpret than linear models and can be computationally expensive.\n",
    "\n",
    "**Support Vector Machines** find optimal boundaries between classes. SVC is for classification and SVR is for regression. They work by finding the hyperplane that maximizes the margin between classes. With different kernels (linear, polynomial, RBF), they can learn complex decision boundaries. SVMs work exceptionally well for medium-sized datasets and are particularly strong when you have more features than samples. However, they scale poorly to very large datasets and require careful feature scaling.\n",
    "\n",
    "**Naive Bayes** classifiers use probability theory. GaussianNB assumes features follow a normal distribution, MultinomialNB works for count data like text, and BernoulliNB is for binary features. These are incredibly fast and work surprisingly well for text classification despite their \"naive\" assumption that features are independent. They need very little training data and train in a single pass, making them perfect for streaming data or when you need quick baseline results.\n",
    "\n",
    "**Nearest neighbors** methods include KNeighborsClassifier and KNeighborsRegressor. These don't really train in the traditional sense but memorize the training data and make predictions based on the closest examples. They're intuitive, naturally handle multi-class problems, and can learn very complex decision boundaries. The downside is prediction is slow because it must search through training data, and they struggle with high-dimensional data due to the curse of dimensionality.\n",
    "\n",
    "**Neural networks** in scikit-learn are represented by MLPClassifier and MLPRegressor, where MLP stands for Multi-Layer Perceptron. These can learn highly complex patterns but require careful tuning of architecture and training parameters. They're useful for moderately complex patterns, though for serious deep learning you'd use dedicated frameworks like PyTorch or TensorFlow.\n",
    "\n",
    "**Ensemble methods** deserve special attention because they're often your best performers. The idea is simple yet powerful: combine multiple models to get better predictions than any single model. VotingClassifier and VotingRegressor combine different types of models through voting or averaging. BaggingClassifier and BaggingRegressor train multiple instances of the same model on different subsets of your data. AdaBoostClassifier and AdaBoostRegressor sequentially train models, giving more weight to examples that previous models got wrong. StackingClassifier trains a meta-model that learns how to best combine predictions from multiple base models. In practice, if you need the highest possible accuracy and can afford the computational cost, ensembles are where you'll often end up.\n",
    "\n",
    "## Unsupervised Learning: Finding Structure Without Labels\n",
    "\n",
    "Beyond supervised learning, scikit-learn excels at finding patterns in unlabeled data.\n",
    "\n",
    "**Clustering** groups similar items together. KMeans is the workhorse, partitioning data into k spherical clusters. It's fast and works well when clusters are roughly equal-sized and spherical. DBSCAN finds clusters of arbitrary shape and automatically identifies outliers, but you need to choose density parameters carefully. AgglomerativeClustering builds a hierarchy of clusters by progressively merging similar groups. MeanShift finds clusters by looking for density peaks without needing to specify the number of clusters upfront. SpectralClustering uses graph theory and works well for complex cluster shapes when you know the number of clusters.\n",
    "\n",
    "The key insight is that different clustering algorithms have different assumptions about what makes a \"cluster.\" KMeans assumes spherical shapes, DBSCAN assumes density-based connectivity, hierarchical methods assume you can build a tree of similarities. Choose based on your data's structure and what makes sense for your domain.\n",
    "\n",
    "**Dimensionality reduction** compresses high-dimensional data while preserving important structure. PCA finds linear combinations of features that capture maximum variance. It's fast and works well when relationships are approximately linear. TruncatedSVD is similar but works with sparse matrices, making it popular for text data. NMF (Non-negative Matrix Factorization) constrains everything to be non-negative, which makes sense for data like images or word counts where you're modeling parts that add up. TSNE creates beautiful 2D or 3D visualizations by preserving local neighborhoods, though it's too slow for large datasets. UMAP is a newer alternative that's faster and often better at preserving global structure.\n",
    "\n",
    "**Anomaly detection** identifies unusual examples. IsolationForest isolates anomalies by randomly splitting data, exploiting the fact that anomalies are rare and different. LocalOutlierFactor compares local density around each point to find outliers. OneClassSVM learns a boundary around normal data. EllipticEnvelope assumes your normal data follows a multivariate Gaussian distribution.\n",
    "\n",
    "## Validation Strategies: Beyond Basic Cross-Validation\n",
    "\n",
    "The way you validate your models dramatically affects how well you can trust their performance estimates. Basic k-fold cross-validation is just the beginning.\n",
    "\n",
    "**StratifiedKFold** ensures each fold has the same class distribution as the full dataset. This is crucial for imbalanced classification problems where random splitting might create folds with very few examples of minority classes. You should almost always use stratified splitting for classification unless you have a specific reason not to.\n",
    "\n",
    "**GroupKFold** and related methods handle grouped data where you can't mix certain examples between train and test. Imagine you're predicting patient outcomes and you have multiple measurements per patient. If some measurements from a patient are in training and others in test, you're leaking information. GroupKFold keeps all examples from each patient together in either train or test, never split across both.\n",
    "\n",
    "**TimeSeriesSplit** respects temporal ordering. Unlike regular cross-validation which randomly shuffles data, this always trains on past data and tests on future data. Each fold uses an expanding window of training data followed by a subsequent test period. This prevents the cardinal sin of time series analysis: training on the future to predict the past.\n",
    "\n",
    "**RepeatedKFold** and RepeatedStratifiedKFold run k-fold cross-validation multiple times with different random seeds. This gives you more robust estimates of model performance by reducing the variance that comes from a single random split.\n",
    "\n",
    "**LeaveOneOut** and LeavePOut** use n-1 or n-p examples for training, testing on the remaining ones, and repeat for all combinations. These give nearly unbiased estimates but are computationally expensive and have high variance. They're mainly useful for small datasets where you can't afford to hold out much data.\n",
    "\n",
    "Here's how you'd use these in practice:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, GroupKFold, TimeSeriesSplit,\n",
    "    cross_validate\n",
    ")\n",
    "\n",
    "# For imbalanced classification\n",
    "cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_validate(model, X, y, cv=cv_stratified, \n",
    "                       scoring=['accuracy', 'f1', 'roc_auc'])\n",
    "\n",
    "# For grouped data (e.g., multiple samples per patient)\n",
    "cv_grouped = GroupKFold(n_splits=5)\n",
    "scores = cross_validate(model, X, y, cv=cv_grouped, groups=patient_ids)\n",
    "\n",
    "# For time series\n",
    "cv_time = TimeSeriesSplit(n_splits=5)\n",
    "scores = cross_validate(model, X, y, cv=cv_time)\n",
    "```\n",
    "\n",
    "Notice I'm using `cross_validate` instead of `cross_val_score`. This returns multiple metrics at once and also provides timing information, which helps you understand the computational cost of your model.\n",
    "\n",
    "## Handling Imbalanced Data: When Classes Aren't Equal\n",
    "\n",
    "Real-world data is often imbalanced. You might have 95% negative examples and 5% positive ones. This creates problems because models can achieve high accuracy by simply predicting the majority class every time.\n",
    "\n",
    "Scikit-learn provides several strategies. Many classifiers have a `class_weight` parameter that you can set to 'balanced' to automatically weight classes inversely proportional to their frequency. This tells the model to pay more attention to minority class examples.\n",
    "\n",
    "```python\n",
    "# The model will treat each class as equally important\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "```\n",
    "\n",
    "You can also manually set class weights if you know the business cost of different errors. Maybe a false negative costs you ten times more than a false positive. You'd set weights accordingly.\n",
    "\n",
    "Another approach is resampling. The imblearn library (which works seamlessly with scikit-learn) provides tools like RandomOverSampler to duplicate minority examples, RandomUnderSampler to remove majority examples, and SMOTE to generate synthetic minority examples. You can incorporate these directly into pipelines.\n",
    "\n",
    "Perhaps most importantly, you need to choose appropriate metrics. Accuracy is meaningless for imbalanced data. Instead, look at precision, recall, F1-score, and particularly ROC-AUC or PR-AUC (precision-recall area under curve). The classification report gives you a comprehensive view.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Get detailed breakdown by class\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# For binary classification with imbalance, PR-AUC is often better than ROC-AUC\n",
    "from sklearn.metrics import average_precision_score\n",
    "score = average_precision_score(y_true, y_pred_proba)\n",
    "```\n",
    "\n",
    "## Advanced Preprocessing: The Full Toolkit\n",
    "\n",
    "Beyond what we covered earlier, scikit-learn has sophisticated preprocessing tools that handle edge cases and special situations.\n",
    "\n",
    "**Handling outliers** matters because extreme values can distort your models. RobustScaler uses the interquartile range instead of mean and standard deviation, making it resistant to outliers. QuantileTransformer maps features to a uniform or normal distribution, which can help with heavily skewed data. PowerTransformer applies Box-Cox or Yeo-Johnson transformations to make data more Gaussian-like.\n",
    "\n",
    "**Binning and discretization** convert continuous features into categorical ones. KBinsDiscretizer splits continuous features into intervals, which can help linear models capture non-linear relationships. Sometimes you know from domain expertise that certain ranges of a continuous variable should be treated categorically, and binning makes this explicit.\n",
    "\n",
    "**Feature engineering transformers** create new features from existing ones. PolynomialFeatures generates interaction terms and polynomial combinations. For example, if you have features x1 and x2, it can create x1², x2², and x1×x2. FunctionTransformer lets you apply any custom function as part of a pipeline, giving you flexibility while maintaining the pipeline structure.\n",
    "\n",
    "**Text processing** has its own specialized tools. CountVectorizer converts text documents into word count matrices. TfidfVectorizer does the same but weights words by their importance (rare words in a document but common across documents get higher weight). HashingVectorizer is memory-efficient for very large vocabularies. These work seamlessly in pipelines alongside other preprocessing.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Text pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Custom transformation in a pipeline\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('log_transform', log_transformer),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "```\n",
    "\n",
    "## Model Inspection: Understanding What Your Model Learned\n",
    "\n",
    "Building models is only half the battle. You need to understand what they've learned to trust them and explain them to others.\n",
    "\n",
    "**Feature importance** tells you which features matter most. Tree-based models provide this naturally through their `feature_importances_` attribute. For linear models, the coefficients tell you feature importance with direction (positive or negative effect). Permutation importance works for any model by shuffling each feature and measuring how much performance drops.\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Works for any model\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': result.importances_mean,\n",
    "    'std': result.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "```\n",
    "\n",
    "**Partial dependence plots** show how predictions change as you vary one or two features while averaging over all others. This reveals the marginal effect of features and can expose non-linear relationships.\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Show how predictions depend on two features\n",
    "features = ['age', 'income']\n",
    "PartialDependenceDisplay.from_estimator(model, X, features)\n",
    "```\n",
    "\n",
    "**Learning curves** plot training and validation scores as you increase training set size. They help diagnose whether you'd benefit from more data (high bias) or simpler models (high variance).\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "# Plot these to see if more data would help\n",
    "```\n",
    "\n",
    "**Validation curves** show how performance changes as you vary a hyperparameter. This helps you understand if you're in the right range for your hyperparameters and whether you're overfitting or underfitting.\n",
    "\n",
    "## Structuring Real ML Projects: The Professional Approach\n",
    "\n",
    "Now let me show you how to structure actual ML code so it's maintainable, reproducible, and professional. This is the difference between scripts that work once and systems you can rely on.\n",
    "\n",
    "**Separate your concerns** by organizing code into logical modules. Have a data loading module, a feature engineering module, a model building module, and an evaluation module. Each should have a clear responsibility and clean interfaces between them.\n",
    "\n",
    "```python\n",
    "# project_structure/\n",
    "# ├── data/\n",
    "# │   ├── raw/\n",
    "# │   └── processed/\n",
    "# ├── notebooks/\n",
    "# │   └── exploration.ipynb\n",
    "# ├── src/\n",
    "# │   ├── __init__.py\n",
    "# │   ├── data.py          # Data loading and splitting\n",
    "# │   ├── features.py      # Feature engineering pipelines\n",
    "# │   ├── models.py        # Model definitions\n",
    "# │   ├── evaluate.py      # Evaluation functions\n",
    "# │   └── train.py         # Training scripts\n",
    "# ├── tests/\n",
    "# │   └── test_features.py\n",
    "# ├── models/\n",
    "# │   └── trained_models/\n",
    "# └── requirements.txt\n",
    "```\n",
    "\n",
    "Your data module handles loading and splitting consistently:\n",
    "\n",
    "```python\n",
    "# src/data.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from CSV with consistent dtypes and validation.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Add validation checks\n",
    "    required_cols = ['feature1', 'feature2', 'target']\n",
    "    assert all(col in df.columns for col in required_cols)\n",
    "    return df\n",
    "\n",
    "def split_data(df, target_col, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"Create train/val/test splits consistently.\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # First split off test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Then split remaining into train and validation\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, \n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "```\n",
    "\n",
    "Your features module defines reusable preprocessing pipelines:\n",
    "\n",
    "```python\n",
    "# src/features.py\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def create_preprocessor(numeric_features, categorical_features):\n",
    "    \"\"\"Create preprocessing pipeline for the dataset.\"\"\"\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'  # Explicitly drop columns not specified\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "```\n",
    "\n",
    "Your models module defines model configurations:\n",
    "\n",
    "```python\n",
    "# src/models.py\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_model_config():\n",
    "    \"\"\"Return dictionary of models to try with their param grids.\"\"\"\n",
    "    return {\n",
    "        'logistic': {\n",
    "            'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'params': {\n",
    "                'classifier__C': [0.1, 1.0, 10.0],\n",
    "                'classifier__penalty': ['l2']\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            'params': {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__max_depth': [10, 20, None],\n",
    "                'classifier__min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__learning_rate': [0.01, 0.1],\n",
    "                'classifier__max_depth': [3, 5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "Your evaluation module provides consistent metrics:\n",
    "\n",
    "```python\n",
    "# src/evaluate.py\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(model, X, y, dataset_name='test'):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'dataset': dataset_name,\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, average='binary'),\n",
    "        'recall': recall_score(y, y_pred, average='binary'),\n",
    "        'f1': f1_score(y, y_pred, average='binary')\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()} SET RESULTS:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    return pd.Series(metrics)\n",
    "```\n",
    "\n",
    "Finally, your training script orchestrates everything:\n",
    "\n",
    "```python\n",
    "# src/train.py\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from data import load_data, split_data\n",
    "from features import create_preprocessor\n",
    "from models import get_model_config\n",
    "from evaluate import evaluate_model\n",
    "\n",
    "def train_and_evaluate(data_path, model_name, output_dir='models'):\n",
    "    \"\"\"Complete training pipeline.\"\"\"\n",
    "    # Load and split data\n",
    "    df = load_data(data_path)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, 'target')\n",
    "    \n",
    "    # Define feature types (could also be loaded from config)\n",
    "    numeric_features = ['age', 'income', 'score']\n",
    "    categorical_features = ['city', 'occupation']\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessor(numeric_features, categorical_features)\n",
    "    \n",
    "    # Get model configuration\n",
    "    model_configs = get_model_config()\n",
    "    config = model_configs[model_name]\n",
    "    \n",
    "    # Create full pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', config['model'])\n",
    "    ])\n",
    "    \n",
    "    # Hyperparameter search with cross-validation on training set\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        config['params'],\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Training {model_name}...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics = evaluate_model(grid_search, X_val, y_val, 'validation')\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    test_metrics = evaluate_model(grid_search, X_test, y_test, 'test')\n",
    "    \n",
    "    # Save the model\n",
    "    output_path = Path(output_dir) / f'{model_name}_model.pkl'\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    joblib.dump(grid_search.best_estimator_, output_path)\n",
    "    print(f\"Model saved to {output_path}\")\n",
    "    \n",
    "    return grid_search, val_metrics, test_metrics\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    \n",
    "    data_path = sys.argv[1] if len(sys.argv) > 1 else 'data/processed/data.csv'\n",
    "    model_name = sys.argv[2] if len(sys.argv) > 2 else 'random_forest'\n",
    "    \n",
    "    train_and_evaluate(data_path, model_name)\n",
    "```\n",
    "\n",
    "This structure makes your work reproducible, testable, and collaborative. Each module has a single responsibility, making debugging easier. You can import these functions in notebooks for exploration or run them as scripts for production training.\n",
    "\n",
    "**Configuration management** is crucial for reproducibility. Use config files (YAML or JSON) to store hyperparameters, feature definitions, and data paths rather than hardcoding them. This makes it easy to experiment with different configurations without changing code.\n",
    "\n",
    "**Model versioning** matters when you're iterating. Save not just the model but also metadata about the data version, features used, hyperparameters, and performance metrics. This lets you reproduce any model you've trained.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_with_metadata(model, metadata, model_name):\n",
    "    \"\"\"Save model along with training metadata.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_path = f'models/{model_name}_{timestamp}.pkl'\n",
    "    metadata_path = f'models/{model_name}_{timestamp}_metadata.json'\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved model to {model_path}\")\n",
    "    print(f\"Saved metadata to {metadata_path}\")\n",
    "```\n",
    "\n",
    "**Testing your pipelines** prevents subtle bugs. Write unit tests for your feature engineering to ensure transformations work as expected. Test that your pipeline handles edge cases like missing values or unseen categories.\n",
    "\n",
    "```python\n",
    "# tests/test_features.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from src.features import create_preprocessor\n",
    "\n",
    "def test_preprocessor_handles_missing_values():\n",
    "    \"\"\"Test that preprocessor correctly imputes missing values.\"\"\"\n",
    "    X = pd.DataFrame({\n",
    "        'numeric_col': [1, None, 3],\n",
    "        'cat_col': ['A', 'B', None]\n",
    "    })\n",
    "    \n",
    "    preprocessor = create_preprocessor(['numeric_col'], ['cat_col'])\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # No NaN values should remain\n",
    "    assert not pd.isna(X_transformed).any()\n",
    "```\n",
    "\n",
    "This professional structure scales from small projects to production systems. The key is consistency, reproducibility, and maintainability.\n",
    "\n",
    "Would you like me to explore any specific area further, such as deployment considerations, working with specific types of data like text or images, or advanced topics like custom estimators and transformers that extend scikit-learn's functionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f766f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025-3.0-py312-tiled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
