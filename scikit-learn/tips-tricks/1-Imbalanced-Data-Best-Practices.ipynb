{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456c8db5",
   "metadata": {},
   "source": [
    "1) **for class imbalanced dataset do we use stratified CV or Handle class imbalance with class weight=’balanced’ ?**\n",
    "\n",
    "2) **I don't understand \"If imbalanced, report PR-AUC alongside ROC-AUC/F1 and give the prevalence baseline for PR-AUC calibrate probabilities if you threshold.\"**\n",
    "\n",
    "3) **why Handle high-cardinality fields with target/frequency encoding inside CV, why not in preprocessing pipeline?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306893d",
   "metadata": {},
   "source": [
    "# Excellent Questions! Let me clarify these important concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Stratified CV vs class_weight='balanced'\n",
    "\n",
    "### They're NOT Alternatives - Use BOTH!\n",
    "\n",
    "**They solve different problems:**\n",
    "\n",
    "```python\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ STRATIFIED CV          → Ensures fair data splitting       │\n",
    "│ class_weight='balanced' → Adjusts model's learning process │\n",
    "│                                                              │\n",
    "│ Use BOTH for imbalanced data! ✅                            │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Stratified CV - What It Does\n",
    "\n",
    "**Problem without stratification:**\n",
    "\n",
    "```python\n",
    "# Dataset: 90% class 0, 10% class 1\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Regular KFold (BAD for imbalanced data!)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold_idx}:\")\n",
    "    print(f\"  Train: {np.bincount(y[train_idx])}\")  \n",
    "    print(f\"  Val:   {np.bincount(y[val_idx])}\")\n",
    "\n",
    "# Output might be:\n",
    "# Fold 0:\n",
    "#   Train: [7200, 800]  # 90% vs 10% ✅\n",
    "#   Val:   [1800, 200]  # 90% vs 10% ✅\n",
    "# \n",
    "# Fold 1:\n",
    "#   Train: [7350, 650]  # 91.9% vs 8.1% ⚠️  Different!\n",
    "#   Val:   [1650, 350]  # 82.5% vs 17.5% ⚠️  Much different!\n",
    "# \n",
    "# Fold 2:\n",
    "#   Train: [7100, 900]  # 88.75% vs 11.25% ⚠️\n",
    "#   Val:   [1900, 100]  # 95% vs 5% ⚠️  Very different!\n",
    "\n",
    "# Each fold has different class distribution → Inconsistent evaluation!\n",
    "```\n",
    "\n",
    "**Solution: Stratified CV**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Stratified CV maintains class distribution\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {fold_idx}:\")\n",
    "    print(f\"  Train: {np.bincount(y[train_idx])}\")  \n",
    "    print(f\"  Val:   {np.bincount(y[val_idx])}\")\n",
    "\n",
    "# Output:\n",
    "# Fold 0:\n",
    "#   Train: [7200, 800]  # 90% vs 10% ✅\n",
    "#   Val:   [1800, 200]  # 90% vs 10% ✅\n",
    "# \n",
    "# Fold 1:\n",
    "#   Train: [7200, 800]  # 90% vs 10% ✅\n",
    "#   Val:   [1800, 200]  # 90% vs 10% ✅\n",
    "# \n",
    "# All folds have same distribution → Fair, consistent evaluation!\n",
    "```\n",
    "\n",
    "**Stratified CV ensures:**\n",
    "- ✅ Each fold has same class distribution\n",
    "- ✅ Fair model comparison across folds\n",
    "- ✅ Prevents one fold from having too few minority samples\n",
    "- ✅ More stable cross-validation scores\n",
    "\n",
    "### class_weight='balanced' - What It Does\n",
    "\n",
    "**Problem: Model ignores minority class**\n",
    "\n",
    "```python\n",
    "# Without class weights\n",
    "# Model learns: \"Just predict majority class to minimize loss\"\n",
    "\n",
    "# Dataset: 9000 class 0, 1000 class 1\n",
    "model = RandomForestClassifier(random_state=42)  # No class_weight\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Loss function treats all errors equally:\n",
    "# Wrong prediction on class 0: Loss = 1\n",
    "# Wrong prediction on class 1: Loss = 1\n",
    "# \n",
    "# Model optimizes: Minimize total loss\n",
    "# → Just predict class 0 always! (9000 correct, 1000 wrong = 90% accuracy!)\n",
    "```\n",
    "\n",
    "**Solution: class_weight='balanced'**\n",
    "\n",
    "```python\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# What 'balanced' does internally:\n",
    "n_samples = len(y_train)  # 10,000\n",
    "n_classes = 2\n",
    "n_samples_per_class = np.bincount(y_train)  # [9000, 1000]\n",
    "\n",
    "# Calculate weights:\n",
    "weight_0 = n_samples / (n_classes * n_samples_per_class[0])\n",
    "         = 10000 / (2 * 9000) = 0.556\n",
    "\n",
    "weight_1 = n_samples / (n_classes * n_samples_per_class[1])\n",
    "         = 10000 / (2 * 1000) = 5.0\n",
    "\n",
    "# Now loss function:\n",
    "# Wrong prediction on class 0: Loss = 1 × 0.556 = 0.556\n",
    "# Wrong prediction on class 1: Loss = 1 × 5.0 = 5.0\n",
    "#\n",
    "# Minority class errors are penalized 9x more!\n",
    "# → Model learns to balance predictions\n",
    "```\n",
    "\n",
    "**class_weight='balanced' makes:**\n",
    "- ✅ Model care about minority class errors\n",
    "- ✅ Adjusts decision boundary\n",
    "- ✅ Improves minority class recall\n",
    "\n",
    "### Use BOTH Together!\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ✅ CORRECT: Use both!\n",
    "model = RandomForestClassifier(\n",
    "    class_weight='balanced',  # Adjust learning\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5, \n",
    "    shuffle=True, \n",
    "    random_state=42  # Fair splitting\n",
    ")\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "print(f\"CV AUC: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "```\n",
    "\n",
    "### Complete Strategy for Imbalanced Data\n",
    "\n",
    "```python\n",
    "# Progression (try in order):\n",
    "\n",
    "# 1. Baseline: class_weight='balanced' + StratifiedKFold\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "# 2. If not enough: Add SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', RandomForestClassifier())  # No class_weight with SMOTE!\n",
    "])\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "# 3. NEVER combine SMOTE + class_weight (redundant!)\n",
    "# ❌ WRONG\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE()),  # Already balances data\n",
    "    ('model', RandomForestClassifier(class_weight='balanced'))  # Double balancing!\n",
    "])\n",
    "# Pick one: SMOTE OR class_weight, not both\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. PR-AUC, Calibration, and Thresholding\n",
    "\n",
    "Let me break down this complex statement piece by piece.\n",
    "\n",
    "### Part 1: \"Report PR-AUC alongside ROC-AUC\"\n",
    "\n",
    "**Why ROC-AUC can be misleading for imbalanced data:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extreme imbalance: 99% class 0, 1% class 1\n",
    "# Model A: Good model\n",
    "# Model B: Bad model (barely better than random)\n",
    "\n",
    "# Dummy example probabilities\n",
    "y_test = np.array([0]*990 + [1]*10)  # 99% class 0, 1% class 1\n",
    "\n",
    "# Model A: Good predictions\n",
    "y_proba_good = np.concatenate([\n",
    "    np.random.beta(2, 5, 990),   # Low probs for class 0\n",
    "    np.random.beta(5, 2, 10)     # High probs for class 1\n",
    "])\n",
    "\n",
    "# Model B: Bad predictions (barely better than random)\n",
    "y_proba_bad = np.concatenate([\n",
    "    np.random.beta(3, 3, 990),   # Medium probs for class 0\n",
    "    np.random.beta(4, 3, 10)     # Slightly higher for class 1\n",
    "])\n",
    "\n",
    "# Evaluate both\n",
    "print(\"Model A (Good):\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_good):.4f}\")  # ~0.95\n",
    "print(f\"  PR-AUC:  {average_precision_score(y_test, y_proba_good):.4f}\")  # ~0.75\n",
    "\n",
    "print(\"\\nModel B (Bad):\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_bad):.4f}\")  # ~0.65 ⚠️ Still looks OK!\n",
    "print(f\"  PR-AUC:  {average_precision_score(y_test, y_proba_bad):.4f}\")  # ~0.15 ⚠️ Reveals it's bad!\n",
    "\n",
    "# ROC-AUC is optimistic because true negatives (990 samples) dominate\n",
    "# PR-AUC focuses on precision and recall (ignores true negatives) → more honest\n",
    "```\n",
    "\n",
    "**Visual difference:**\n",
    "\n",
    "```python\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr_good, tpr_good, _ = roc_curve(y_test, y_proba_good)\n",
    "fpr_bad, tpr_bad, _ = roc_curve(y_test, y_proba_bad)\n",
    "\n",
    "axes[0].plot(fpr_good, tpr_good, label=f'Good (AUC={roc_auc_score(y_test, y_proba_good):.3f})')\n",
    "axes[0].plot(fpr_bad, tpr_bad, label=f'Bad (AUC={roc_auc_score(y_test, y_proba_bad):.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve (Optimistic for Imbalanced Data)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "prec_good, rec_good, _ = precision_recall_curve(y_test, y_proba_good)\n",
    "prec_bad, rec_bad, _ = precision_recall_curve(y_test, y_proba_bad)\n",
    "\n",
    "axes[1].plot(rec_good, prec_good, label=f'Good (AP={average_precision_score(y_test, y_proba_good):.3f})')\n",
    "axes[1].plot(rec_bad, prec_bad, label=f'Bad (AP={average_precision_score(y_test, y_proba_bad):.3f})')\n",
    "axes[1].axhline(y=0.01, color='k', linestyle='--', label='Baseline (prevalence)')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('PR Curve (Honest for Imbalanced Data)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Key insight:**\n",
    "- **ROC-AUC**: Looks at FPR (false positives / all negatives)\n",
    "  - With 990 negatives, even 100 false positives = 10% FPR (looks good!)\n",
    "- **PR-AUC**: Looks at precision (true positives / all predicted positives)\n",
    "  - If you predict 110 positives (10 TP + 100 FP), precision = 10/110 = 9% (terrible!)\n",
    "\n",
    "### Part 2: \"Give the prevalence baseline for PR-AUC\"\n",
    "\n",
    "**Prevalence baseline** = Performance of a dummy classifier that predicts positive randomly at the rate of the minority class.\n",
    "\n",
    "```python\n",
    "# Calculate prevalence (base rate of minority class)\n",
    "prevalence = (y_test == 1).sum() / len(y_test)\n",
    "print(f\"Prevalence (minority class rate): {prevalence:.2%}\")  # 1%\n",
    "\n",
    "# Prevalence baseline for PR-AUC\n",
    "# A random classifier gets precision = prevalence\n",
    "# Example: If you predict randomly, 1% of predictions will be correct\n",
    "\n",
    "# Your model's PR-AUC MUST beat this baseline!\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "print(f\"Model PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"Baseline:     {prevalence:.4f}\")\n",
    "\n",
    "if pr_auc > prevalence:\n",
    "    print(f\"✅ Model is {pr_auc/prevalence:.1f}x better than random!\")\n",
    "else:\n",
    "    print(\"❌ Model is useless (no better than random)\")\n",
    "```\n",
    "\n",
    "**Always report both:**\n",
    "\n",
    "```python\n",
    "# Complete evaluation for imbalanced data\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION (Imbalanced Data: 99% vs 1%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prevalence = (y_test == 1).mean()\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Class 0: {(y_test == 0).sum()} ({(y_test == 0).mean():.1%})\")\n",
    "print(f\"  Class 1: {(y_test == 1).sum()} ({prevalence:.1%})\")\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"  PR-AUC:  {average_precision_score(y_test, y_proba):.4f}\")\n",
    "print(f\"  Baseline (prevalence): {prevalence:.4f}\")\n",
    "print(f\"  PR-AUC improvement: {average_precision_score(y_test, y_proba)/prevalence:.1f}x over random\")\n",
    "```\n",
    "\n",
    "### Part 3: \"Calibrate probabilities if you threshold\"\n",
    "\n",
    "**What is thresholding?**\n",
    "\n",
    "Default prediction uses 0.5 threshold:\n",
    "```python\n",
    "# Default behavior\n",
    "y_pred = model.predict(X_test)\n",
    "# Internally: y_pred = (model.predict_proba(X_test)[:, 1] >= 0.5).astype(int)\n",
    "```\n",
    "\n",
    "**Custom thresholding** = Adjust the cutoff:\n",
    "```python\n",
    "# Custom threshold\n",
    "threshold = 0.3  # Predict positive if probability >= 30%\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred_custom = (y_proba >= threshold).astype(int)\n",
    "\n",
    "# Why? To optimize for your use case:\n",
    "# - Fraud detection: Low threshold (catch more fraud, accept false alarms)\n",
    "# - Spam filter: High threshold (avoid false positives)\n",
    "```\n",
    "\n",
    "**Why calibration matters for thresholding:**\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Example: Uncalibrated model (e.g., SVM or Naive Bayes)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_proba_uncal = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Problem: Probabilities are overconfident\n",
    "# Model says 0.7 but true rate is only 0.4\n",
    "# Your threshold of 0.3 won't work as intended!\n",
    "\n",
    "# Solution: Calibrate\n",
    "calibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
    "calibrated.fit(X_train, y_train)\n",
    "y_proba_cal = calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Now probabilities are trustworthy\n",
    "# 0.7 actually means 70% chance\n",
    "# Thresholding works as expected\n",
    "```\n",
    "\n",
    "**When to calibrate:**\n",
    "\n",
    "```python\n",
    "# You need calibration if:\n",
    "# 1. You're using thresholds (not just default 0.5)\n",
    "# 2. Probabilities matter for decision-making\n",
    "# 3. Using models known to be poorly calibrated:\n",
    "#    - Naive Bayes (very overconfident)\n",
    "#    - SVMs (not designed for probabilities)\n",
    "#    - Boosted trees (often miscalibrated)\n",
    "\n",
    "# Example: Cost-sensitive decision\n",
    "def make_decision(proba, threshold=0.3):\n",
    "    \"\"\"\n",
    "    threshold=0.3: Only act if >30% confident\n",
    "    Requires calibrated probabilities!\n",
    "    \"\"\"\n",
    "    return proba >= threshold\n",
    "\n",
    "# Uncalibrated model says 0.4 → You act → But true probability was 0.2 → Bad decision!\n",
    "# Calibrated model says 0.2 → You don't act → Correct!\n",
    "```\n",
    "\n",
    "**Complete example:**\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Train uncalibrated model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calibrate it\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    model, \n",
    "    method='sigmoid',\n",
    "    cv=StratifiedKFold(5, shuffle=True, random_state=42)\n",
    ")\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# Get probabilities\n",
    "y_proba_uncal = model.predict_proba(X_test)[:, 1]\n",
    "y_proba_cal = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate calibration\n",
    "brier_uncal = brier_score_loss(y_test, y_proba_uncal)\n",
    "brier_cal = brier_score_loss(y_test, y_proba_cal)\n",
    "\n",
    "print(f\"Brier score (uncalibrated): {brier_uncal:.4f}\")\n",
    "print(f\"Brier score (calibrated):   {brier_cal:.4f}\")\n",
    "print(f\"Improvement: {brier_uncal - brier_cal:.4f}\")\n",
    "\n",
    "# Use custom threshold with calibrated probabilities\n",
    "threshold = 0.2  # Lower threshold for imbalanced data\n",
    "\n",
    "y_pred_uncal = (y_proba_uncal >= threshold).astype(int)\n",
    "y_pred_cal = (y_proba_cal >= threshold).astype(int)\n",
    "\n",
    "print(f\"\\nWith threshold={threshold}:\")\n",
    "print(f\"Uncalibrated recall: {recall_score(y_test, y_pred_uncal):.3f}\")\n",
    "print(f\"Calibrated recall:   {recall_score(y_test, y_pred_cal):.3f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Target/Frequency Encoding Inside CV vs Preprocessing\n",
    "\n",
    "### The Data Leakage Problem\n",
    "\n",
    "**Target encoding** uses the target variable to encode features:\n",
    "\n",
    "```python\n",
    "# Example: Customer ID with purchase history\n",
    "# customer_id | purchased (target)\n",
    "# ------------|-------------------\n",
    "# A           | 1\n",
    "# A           | 1\n",
    "# A           | 0\n",
    "# B           | 0\n",
    "# B           | 0\n",
    "# C           | 1\n",
    "\n",
    "# Target encoding: Replace customer_id with mean purchase rate\n",
    "# customer_id → purchase_rate\n",
    "# A → 0.67 (2/3 purchases)\n",
    "# B → 0.00 (0/2 purchases)\n",
    "# C → 1.00 (1/1 purchases)\n",
    "```\n",
    "\n",
    "**❌ WRONG: Encoding before CV**\n",
    "\n",
    "```python\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# ❌ DATA LEAKAGE!\n",
    "encoder = TargetEncoder()\n",
    "X_encoded = encoder.fit_transform(X_train, y_train)  # Uses ALL training data!\n",
    "\n",
    "# Now do CV\n",
    "cv_scores = cross_val_score(model, X_encoded, y_train, cv=5)\n",
    "\n",
    "# PROBLEM: Each fold's validation set contains information from its own target values!\n",
    "```\n",
    "\n",
    "**What went wrong:**\n",
    "\n",
    "```python\n",
    "# Let's trace what happens:\n",
    "\n",
    "# Original data (before split):\n",
    "# customer_id | purchased\n",
    "# A           | 1\n",
    "# A           | 1  ← Fold 1 validation\n",
    "# A           | 0\n",
    "# B           | 0\n",
    "# B           | 0  ← Fold 1 validation\n",
    "# C           | 1\n",
    "\n",
    "# You encoded BEFORE CV split:\n",
    "encoder.fit(X_train, y_train)  # Uses ALL data including fold 1 validation!\n",
    "# A → 0.67 (calculated using the validation row too!)\n",
    "# B → 0.00 (calculated using the validation row too!)\n",
    "\n",
    "# Now in Fold 1:\n",
    "# Validation set:\n",
    "#   A (encoded as 0.67) | actual=1  ← This 0.67 was calculated USING this row!\n",
    "#   B (encoded as 0.00) | actual=0  ← This 0.00 was calculated USING this row!\n",
    "#\n",
    "# The model is cheating! It saw the validation labels during encoding!\n",
    "```\n",
    "\n",
    "**✅ CORRECT: Encoding inside CV (via Pipeline)**\n",
    "\n",
    "```python\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ✅ NO LEAKAGE\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder()),  # Encoding happens INSIDE each fold\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "# What happens in each fold:\n",
    "# Fold 1:\n",
    "#   1. Split data → train_fold, val_fold\n",
    "#   2. Fit encoder on train_fold only → A=0.50, B=0.00, C=1.00\n",
    "#   3. Transform train_fold with these encodings\n",
    "#   4. Transform val_fold with these encodings (never saw val_fold targets!)\n",
    "#   5. Train model on encoded train_fold\n",
    "#   6. Evaluate on encoded val_fold (clean!)\n",
    "#\n",
    "# Fold 2:\n",
    "#   1. Different split...\n",
    "#   2. Refit encoder (different encodings this time!)\n",
    "#   ...\n",
    "```\n",
    "\n",
    "### Visualizing the Difference\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C'],\n",
    "    'purchased': [1, 1, 0, 0, 0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "X = df[['customer_id']]\n",
    "y = df['purchased']\n",
    "\n",
    "cv = StratifiedKFold(n_splits=2, shuffle=False)  # For illustration\n",
    "\n",
    "# ❌ WRONG: Fit on all data first\n",
    "encoder_wrong = TargetEncoder()\n",
    "encoder_wrong.fit(X, y)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WRONG WAY (Data Leakage)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEncoder fitted on ALL data:\")\n",
    "print(encoder_wrong.transform(X))\n",
    "# All A's get same encoding (0.67) calculated from all A's including validation!\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    print(f\"\\nFold {fold_idx}:\")\n",
    "    print(f\"  Validation indices: {val_idx}\")\n",
    "    print(f\"  Validation data:\")\n",
    "    print(df.iloc[val_idx])\n",
    "    print(f\"  Encoded values used: {encoder_wrong.transform(X.iloc[val_idx]).values.flatten()}\")\n",
    "    print(\"  ⚠️  These encodings were calculated USING validation targets!\")\n",
    "\n",
    "# ✅ CORRECT: Fit only on training fold\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRECT WAY (No Leakage)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    # Fit encoder ONLY on training fold\n",
    "    encoder_correct = TargetEncoder()\n",
    "    encoder_correct.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx}:\")\n",
    "    print(f\"  Training indices: {train_idx}\")\n",
    "    print(f\"  Training data:\")\n",
    "    print(df.iloc[train_idx])\n",
    "    \n",
    "    print(f\"\\n  Encoder fitted on training only:\")\n",
    "    print(encoder_correct.transform(X.iloc[train_idx]))\n",
    "    \n",
    "    print(f\"\\n  Validation indices: {val_idx}\")\n",
    "    print(f\"  Validation data:\")\n",
    "    print(df.iloc[val_idx])\n",
    "    print(f\"  Encoded values: {encoder_correct.transform(X.iloc[val_idx]).values.flatten()}\")\n",
    "    print(\"  ✅ These encodings were calculated WITHOUT seeing validation targets!\")\n",
    "```\n",
    "\n",
    "### Why Pipeline Prevents Leakage\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ✅ Pipeline automatically does the right thing\n",
    "pipeline = Pipeline([\n",
    "    ('target_encoder', TargetEncoder(cols=['customer_id'])),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# During cross_val_score, for each fold:\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=StratifiedKFold(5))\n",
    "\n",
    "# Internally, sklearn does:\n",
    "for train_idx, val_idx in cv.split(X, y):\n",
    "    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Clone the pipeline for this fold\n",
    "    fold_pipeline = clone(pipeline)\n",
    "    \n",
    "    # Fit ONLY on training fold\n",
    "    fold_pipeline.fit(X_train_fold, y_train_fold)\n",
    "    # ↑ This fits encoder on training fold only!\n",
    "    \n",
    "    # Evaluate on validation fold\n",
    "    score = fold_pipeline.score(X_val_fold, y_val_fold)\n",
    "    # ↑ Validation fold was never seen during encoding!\n",
    "```\n",
    "\n",
    "### Other Encodings That Need Pipeline\n",
    "\n",
    "```python\n",
    "# Any encoding that uses target variable MUST be in pipeline:\n",
    "\n",
    "# ❌ WRONG (leakage)\n",
    "from category_encoders import TargetEncoder, CatBoostEncoder\n",
    "encoder = TargetEncoder()\n",
    "X_encoded = encoder.fit_transform(X_train, y_train)\n",
    "cross_val_score(model, X_encoded, y_train, cv=5)  # Leakage!\n",
    "\n",
    "# ✅ CORRECT\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder()),\n",
    "    ('model', model)\n",
    "])\n",
    "cross_val_score(pipeline, X_train, y_train, cv=5)  # Safe!\n",
    "\n",
    "# Same for:\n",
    "# - TargetEncoder\n",
    "# - CatBoostEncoder  \n",
    "# - WOEEncoder (Weight of Evidence)\n",
    "# - JamesSteinEncoder\n",
    "# - MEstimateEncoder\n",
    "```\n",
    "\n",
    "### Encodings Safe in Preprocessing (No Leakage)\n",
    "\n",
    "```python\n",
    "# These DON'T use target, so safe to do before CV:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# ✅ Safe: OneHotEncoding (doesn't use y)\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = ohe.fit_transform(X_train)  # No y needed!\n",
    "cross_val_score(model, X_encoded, y_train, cv=5)  # No leakage\n",
    "\n",
    "# ✅ Safe: Frequency encoding (doesn't use y)\n",
    "def frequency_encode(X):\n",
    "    freq_map = X['customer_id'].value_counts().to_dict()\n",
    "    return X['customer_id'].map(freq_map)\n",
    "\n",
    "X_encoded = frequency_encode(X_train)\n",
    "cross_val_score(model, X_encoded, y_train, cv=5)  # No leakage\n",
    "\n",
    "# But STILL better to use Pipeline for consistency!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### 1. Stratified CV vs class_weight='balanced'\n",
    "\n",
    "```python\n",
    "# ✅ USE BOTH!\n",
    "model = RandomForestClassifier(class_weight='balanced')  # Adjusts learning\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)   # Fair splitting\n",
    "\n",
    "# They solve different problems:\n",
    "# - Stratified CV: Ensures fair data splitting\n",
    "# - class_weight: Adjusts model's loss function\n",
    "```\n",
    "\n",
    "### 2. Imbalanced Data Evaluation\n",
    "\n",
    "```python\n",
    "# ✅ ALWAYS REPORT BOTH ROC-AUC AND PR-AUC\n",
    "prevalence = (y_test == 1).mean()\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"PR-AUC:  {average_precision_score(y_test, y_proba):.4f}\")\n",
    "print(f\"Baseline (prevalence): {prevalence:.4f}\")\n",
    "print(f\"Improvement over random: {average_precision_score(y_test, y_proba)/prevalence:.1f}x\")\n",
    "\n",
    "# ✅ CALIBRATE if using custom thresholds\n",
    "calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
    "y_proba = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_proba >= 0.3).astype(int)  # Custom threshold\n",
    "```\n",
    "\n",
    "### 3. Target Encoding\n",
    "\n",
    "```python\n",
    "# ❌ WRONG - Data leakage\n",
    "encoder = TargetEncoder()\n",
    "X_encoded = encoder.fit_transform(X_train, y_train)\n",
    "cross_val_score(model, X_encoded, y_train, cv=5)  # LEAKAGE!\n",
    "\n",
    "# ✅ CORRECT - Use Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder()),\n",
    "    ('model', model)\n",
    "])\n",
    "cross_val_score(pipeline, X_train, y_train, cv=5)  # Safe!\n",
    "```\n",
    "\n",
    "Want me to show:\n",
    "1. **Complete example combining all three concepts**?\n",
    "2. **How to find optimal threshold** for imbalanced data?\n",
    "3. **More advanced target encoding techniques** (smoothing, leave-one-out)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082695ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
