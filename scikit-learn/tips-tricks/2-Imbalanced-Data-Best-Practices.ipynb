{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce91f53",
   "metadata": {},
   "source": [
    "# Complete Example: Imbalanced Data Best Practices\n",
    "\n",
    "I'll create a comprehensive example that combines:\n",
    "1. **Stratified CV** + **class_weight='balanced'**\n",
    "2. **Target encoding inside Pipeline** (no leakage)\n",
    "3. **ROC-AUC + PR-AUC** with prevalence baseline\n",
    "4. **Calibration** for thresholding\n",
    "5. **Optimal threshold selection**\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, \n",
    "    roc_curve, precision_recall_curve,\n",
    "    classification_report, confusion_matrix,\n",
    "    brier_score_loss, recall_score, precision_score, f1_score\n",
    ")\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: CREATE IMBALANCED DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: CREATE IMBALANCED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create imbalanced dataset (95% vs 5%)\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.95, 0.05],  # 95% class 0, 5% class 1\n",
    "    flip_y=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add high-cardinality categorical feature\n",
    "# (Simulating customer_id, product_id, etc.)\n",
    "np.random.seed(42)\n",
    "n_categories = 100\n",
    "categorical_feature = np.random.choice([f'CAT_{i}' for i in range(n_categories)], size=len(y))\n",
    "\n",
    "# Make categories correlated with target\n",
    "# Categories 0-30: Higher purchase rate\n",
    "# Categories 31-99: Lower purchase rate\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 1:  # Minority class\n",
    "        # More likely to be in first 30 categories\n",
    "        categorical_feature[i] = f'CAT_{np.random.randint(0, 40)}'\n",
    "    else:  # Majority class\n",
    "        # More likely to be in categories 20-99\n",
    "        categorical_feature[i] = f'CAT_{np.random.randint(20, 100)}'\n",
    "\n",
    "# Create DataFrame\n",
    "X_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "X_df['customer_id'] = categorical_feature\n",
    "\n",
    "print(f\"Dataset shape: {X_df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Class 0 (majority): {(y == 0).sum()} ({(y == 0).mean():.1%})\")\n",
    "print(f\"  Class 1 (minority): {(y == 1).sum()} ({(y == 1).mean():.1%})\")\n",
    "\n",
    "prevalence = (y == 1).mean()\n",
    "imbalance_ratio = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "print(f\"Prevalence (minority class rate): {prevalence:.2%}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: TRAIN-TEST SPLIT (STRATIFIED!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: STRATIFIED TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,  # âœ… Maintain class distribution!\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  Total: {len(y_train)}\")\n",
    "print(f\"  Class 0: {(y_train == 0).sum()} ({(y_train == 0).mean():.1%})\")\n",
    "print(f\"  Class 1: {(y_train == 1).sum()} ({(y_train == 1).mean():.1%})\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Total: {len(y_test)}\")\n",
    "print(f\"  Class 0: {(y_test == 0).sum()} ({(y_test == 0).mean():.1%})\")\n",
    "print(f\"  Class 1: {(y_test == 1).sum()} ({(y_test == 1).mean():.1%})\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: BUILD PIPELINE (Target Encoding INSIDE to prevent leakage!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: BUILD PIPELINE (Target Encoding Inside)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = [col for col in X_train.columns if col.startswith('feature_')]\n",
    "categorical_cols = ['customer_id']\n",
    "\n",
    "print(f\"\\nNumerical features: {len(numerical_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"Categorical cardinality: {X_train['customer_id'].nunique()} unique values\")\n",
    "\n",
    "# âœ… CORRECT: Target encoding in Pipeline\n",
    "# This prevents data leakage during cross-validation!\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(), categorical_cols)  # âœ… Inside pipeline!\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Complete pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        class_weight='balanced',  # âœ… Handle imbalance\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\nâœ… Pipeline created:\")\n",
    "print(pipeline)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: CROSS-VALIDATION (Stratified CV!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: CROSS-VALIDATION (Stratified)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# âœ… Stratified CV for imbalanced data\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify stratification works\n",
    "print(\"\\nVerifying stratification in each fold:\")\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    y_train_fold = y_train.iloc[train_idx]\n",
    "    y_val_fold = y_train.iloc[val_idx]\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "    print(f\"  Train: {len(train_idx)} samples, \"\n",
    "          f\"Class 1: {(y_train_fold == 1).sum()} ({(y_train_fold == 1).mean():.1%})\")\n",
    "    print(f\"  Val:   {len(val_idx)} samples, \"\n",
    "          f\"Class 1: {(y_val_fold == 1).sum()} ({(y_val_fold == 1).mean():.1%})\")\n",
    "\n",
    "# Cross-validate with multiple metrics\n",
    "print(\"\\nRunning cross-validation...\")\n",
    "\n",
    "scoring = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'pr_auc': 'average_precision',  # PR-AUC\n",
    "    'recall': 'recall',\n",
    "    'precision': 'precision',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    pipeline, X_train, y_train,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for metric_name, metric_key in [('ROC-AUC', 'test_roc_auc'), \n",
    "                                  ('PR-AUC', 'test_pr_auc'),\n",
    "                                  ('Recall', 'test_recall'),\n",
    "                                  ('Precision', 'test_precision'),\n",
    "                                  ('F1', 'test_f1')]:\n",
    "    scores = cv_results[metric_key]\n",
    "    print(f\"{metric_name:12s}: {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "\n",
    "# Compare PR-AUC to prevalence baseline\n",
    "pr_auc_mean = cv_results['test_pr_auc'].mean()\n",
    "print(f\"\\nPR-AUC vs Baseline:\")\n",
    "print(f\"  Model PR-AUC:      {pr_auc_mean:.4f}\")\n",
    "print(f\"  Baseline (random): {prevalence:.4f}\")\n",
    "print(f\"  Improvement:       {pr_auc_mean/prevalence:.1f}x better than random âœ…\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TRAIN FINAL MODEL & CALIBRATE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: TRAIN FINAL MODEL & CALIBRATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train uncalibrated model\n",
    "print(\"\\nTraining uncalibrated model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Apply calibration\n",
    "print(\"Calibrating model...\")\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    pipeline,\n",
    "    method='sigmoid',  # Platt scaling\n",
    "    cv=cv,  # Use same CV strategy\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "print(\"âœ… Calibration complete\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: EVALUATE CALIBRATION QUALITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: EVALUATE CALIBRATION QUALITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions\n",
    "y_proba_uncal = pipeline.predict_proba(X_test)[:, 1]\n",
    "y_proba_cal = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Brier score (lower is better)\n",
    "brier_uncal = brier_score_loss(y_test, y_proba_uncal)\n",
    "brier_cal = brier_score_loss(y_test, y_proba_cal)\n",
    "\n",
    "print(f\"\\nBrier Score (lower is better):\")\n",
    "print(f\"  Uncalibrated: {brier_uncal:.4f}\")\n",
    "print(f\"  Calibrated:   {brier_cal:.4f}\")\n",
    "print(f\"  Improvement:  {brier_uncal - brier_cal:.4f}\")\n",
    "\n",
    "# Plot calibration curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calibration curve\n",
    "fraction_pos_uncal, mean_pred_uncal = calibration_curve(\n",
    "    y_test, y_proba_uncal, n_bins=10, strategy='uniform'\n",
    ")\n",
    "fraction_pos_cal, mean_pred_cal = calibration_curve(\n",
    "    y_test, y_proba_cal, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "axes[0].plot(mean_pred_uncal, fraction_pos_uncal, 's-', \n",
    "             label=f'Before calibration (Brier={brier_uncal:.3f})', linewidth=2)\n",
    "axes[0].plot(mean_pred_cal, fraction_pos_cal, 'o-', \n",
    "             label=f'After calibration (Brier={brier_cal:.3f})', linewidth=2)\n",
    "axes[0].set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "axes[0].set_ylabel('Fraction of Positives', fontsize=12)\n",
    "axes[0].set_title('Calibration Curve', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Probability distributions\n",
    "axes[1].hist(y_proba_uncal, bins=20, alpha=0.5, label='Uncalibrated', edgecolor='black')\n",
    "axes[1].hist(y_proba_cal, bins=20, alpha=0.5, label='Calibrated', edgecolor='black')\n",
    "axes[1].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Default threshold')\n",
    "axes[1].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Probability Distributions', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('calibration_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nâœ… Calibration plots saved to 'calibration_analysis.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: EVALUATE WITH ROC-AUC AND PR-AUC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: MODEL EVALUATION (Test Set)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate metrics\n",
    "test_roc_auc = roc_auc_score(y_test, y_proba_cal)\n",
    "test_pr_auc = average_precision_score(y_test, y_proba_cal)\n",
    "test_prevalence = (y_test == 1).mean()\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  ROC-AUC:  {test_roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:   {test_pr_auc:.4f}\")\n",
    "print(f\"\\nBaseline Comparison:\")\n",
    "print(f\"  Prevalence (random baseline): {test_prevalence:.4f}\")\n",
    "print(f\"  PR-AUC improvement: {test_pr_auc/test_prevalence:.1f}x over random âœ…\")\n",
    "\n",
    "# Plot ROC and PR curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_cal)\n",
    "axes[0].plot(fpr, tpr, linewidth=2, label=f'Model (AUC = {test_roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random (AUC = 0.500)')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curve', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba_cal)\n",
    "axes[1].plot(recall_vals, precision_vals, linewidth=2, \n",
    "             label=f'Model (AP = {test_pr_auc:.3f})')\n",
    "axes[1].axhline(y=test_prevalence, color='k', linestyle='--', linewidth=2,\n",
    "                label=f'Random (AP = {test_prevalence:.3f})')\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_pr_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"âœ… ROC and PR curves saved to 'roc_pr_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: FIND OPTIMAL THRESHOLD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: FIND OPTIMAL THRESHOLD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Method 1: Maximize F1 Score\n",
    "precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y_test, y_proba_cal)\n",
    "\n",
    "# Calculate F1 for each threshold\n",
    "f1_scores = 2 * (precision_vals[:-1] * recall_vals[:-1]) / (precision_vals[:-1] + recall_vals[:-1] + 1e-10)\n",
    "optimal_idx_f1 = np.argmax(f1_scores)\n",
    "optimal_threshold_f1 = thresholds_pr[optimal_idx_f1]\n",
    "\n",
    "print(f\"\\nMethod 1: Maximize F1 Score\")\n",
    "print(f\"  Optimal threshold: {optimal_threshold_f1:.3f}\")\n",
    "print(f\"  F1 score:          {f1_scores[optimal_idx_f1]:.4f}\")\n",
    "print(f\"  Precision:         {precision_vals[optimal_idx_f1]:.4f}\")\n",
    "print(f\"  Recall:            {recall_vals[optimal_idx_f1]:.4f}\")\n",
    "\n",
    "# Method 2: Youden's J statistic (maximize TPR - FPR)\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba_cal)\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx_j = np.argmax(j_scores)\n",
    "optimal_threshold_j = thresholds_roc[optimal_idx_j]\n",
    "\n",
    "print(f\"\\nMethod 2: Youden's J Statistic (ROC-based)\")\n",
    "print(f\"  Optimal threshold: {optimal_threshold_j:.3f}\")\n",
    "print(f\"  J statistic:       {j_scores[optimal_idx_j]:.4f}\")\n",
    "print(f\"  TPR (Recall):      {tpr[optimal_idx_j]:.4f}\")\n",
    "print(f\"  FPR:               {fpr[optimal_idx_j]:.4f}\")\n",
    "\n",
    "# Method 3: Target specific recall (e.g., 80%)\n",
    "target_recall = 0.80\n",
    "recall_diff = np.abs(recall_vals[:-1] - target_recall)\n",
    "optimal_idx_recall = np.argmin(recall_diff)\n",
    "optimal_threshold_recall = thresholds_pr[optimal_idx_recall]\n",
    "\n",
    "print(f\"\\nMethod 3: Target Recall = {target_recall:.0%}\")\n",
    "print(f\"  Optimal threshold: {optimal_threshold_recall:.3f}\")\n",
    "print(f\"  Achieved recall:   {recall_vals[optimal_idx_recall]:.4f}\")\n",
    "print(f\"  Precision:         {precision_vals[optimal_idx_recall]:.4f}\")\n",
    "print(f\"  F1 score:          {f1_scores[optimal_idx_recall]:.4f}\")\n",
    "\n",
    "# Method 4: Cost-sensitive threshold\n",
    "# Assume: Cost of False Negative = 10, Cost of False Positive = 1\n",
    "cost_fp = 1\n",
    "cost_fn = 10\n",
    "\n",
    "def calculate_cost(threshold, y_true, y_proba, cost_fp, cost_fn):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    total_cost = fp * cost_fp + fn * cost_fn\n",
    "    return total_cost\n",
    "\n",
    "# Try different thresholds\n",
    "threshold_range = np.linspace(0.01, 0.99, 100)\n",
    "costs = [calculate_cost(t, y_test, y_proba_cal, cost_fp, cost_fn) for t in threshold_range]\n",
    "optimal_idx_cost = np.argmin(costs)\n",
    "optimal_threshold_cost = threshold_range[optimal_idx_cost]\n",
    "\n",
    "print(f\"\\nMethod 4: Cost-Sensitive (FP cost={cost_fp}, FN cost={cost_fn})\")\n",
    "print(f\"  Optimal threshold: {optimal_threshold_cost:.3f}\")\n",
    "print(f\"  Total cost:        {costs[optimal_idx_cost]:.0f}\")\n",
    "\n",
    "# Visualize threshold selection\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# F1 vs Threshold\n",
    "axes[0, 0].plot(thresholds_pr, f1_scores, linewidth=2)\n",
    "axes[0, 0].axvline(optimal_threshold_f1, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Optimal = {optimal_threshold_f1:.3f}')\n",
    "axes[0, 0].axvline(0.5, color='gray', linestyle=':', linewidth=2, label='Default = 0.5')\n",
    "axes[0, 0].set_xlabel('Threshold', fontsize=11)\n",
    "axes[0, 0].set_ylabel('F1 Score', fontsize=11)\n",
    "axes[0, 0].set_title('Method 1: Maximize F1 Score', fontsize=12)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Youden's J vs Threshold\n",
    "axes[0, 1].plot(thresholds_roc, j_scores, linewidth=2)\n",
    "axes[0, 1].axvline(optimal_threshold_j, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Optimal = {optimal_threshold_j:.3f}')\n",
    "axes[0, 1].axvline(0.5, color='gray', linestyle=':', linewidth=2, label='Default = 0.5')\n",
    "axes[0, 1].set_xlabel('Threshold', fontsize=11)\n",
    "axes[0, 1].set_ylabel(\"Youden's J (TPR - FPR)\", fontsize=11)\n",
    "axes[0, 1].set_title(\"Method 2: Youden's J Statistic\", fontsize=12)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall vs Threshold\n",
    "axes[1, 0].plot(thresholds_pr, precision_vals[:-1], label='Precision', linewidth=2)\n",
    "axes[1, 0].plot(thresholds_pr, recall_vals[:-1], label='Recall', linewidth=2)\n",
    "axes[1, 0].axvline(optimal_threshold_recall, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Target Recall = {target_recall:.0%}')\n",
    "axes[1, 0].axvline(0.5, color='gray', linestyle=':', linewidth=2, label='Default = 0.5')\n",
    "axes[1, 0].set_xlabel('Threshold', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Score', fontsize=11)\n",
    "axes[1, 0].set_title(f'Method 3: Target Recall = {target_recall:.0%}', fontsize=12)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Cost vs Threshold\n",
    "axes[1, 1].plot(threshold_range, costs, linewidth=2)\n",
    "axes[1, 1].axvline(optimal_threshold_cost, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Optimal = {optimal_threshold_cost:.3f}')\n",
    "axes[1, 1].axvline(0.5, color='gray', linestyle=':', linewidth=2, label='Default = 0.5')\n",
    "axes[1, 1].set_xlabel('Threshold', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Total Cost', fontsize=11)\n",
    "axes[1, 1].set_title(f'Method 4: Cost-Sensitive (FP={cost_fp}, FN={cost_fn})', fontsize=12)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_selection.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nâœ… Threshold selection plots saved to 'threshold_selection.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: COMPARE THRESHOLDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 9: COMPARE DIFFERENT THRESHOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "thresholds_to_compare = {\n",
    "    'Default (0.5)': 0.5,\n",
    "    'Maximize F1': optimal_threshold_f1,\n",
    "    \"Youden's J\": optimal_threshold_j,\n",
    "    f'Target Recall {target_recall:.0%}': optimal_threshold_recall,\n",
    "    'Cost-Sensitive': optimal_threshold_cost\n",
    "}\n",
    "\n",
    "print(\"\\nThreshold Comparison:\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Method':<25} {'Threshold':>10} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Cost':>10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for method_name, threshold in thresholds_to_compare.items():\n",
    "    y_pred = (y_proba_cal >= threshold).astype(int)\n",
    "    \n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    cost = calculate_cost(threshold, y_test, y_proba_cal, cost_fp, cost_fn)\n",
    "    \n",
    "    print(f\"{method_name:<25} {threshold:>10.3f} {prec:>10.4f} {rec:>10.4f} {f1:>10.4f} {cost:>10.0f}\")\n",
    "\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: FINAL EVALUATION WITH OPTIMAL THRESHOLD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 10: FINAL EVALUATION (Using F1-Optimal Threshold)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use F1-optimal threshold for final evaluation\n",
    "final_threshold = optimal_threshold_f1\n",
    "y_pred_final = (y_proba_cal >= final_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nFinal Threshold: {final_threshold:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, \n",
    "                          target_names=['Majority (0)', 'Minority (1)']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(cm)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nDetailed Breakdown:\")\n",
    "print(f\"  True Negatives:  {tn:5d}\")\n",
    "print(f\"  False Positives: {fp:5d}\")\n",
    "print(f\"  False Negatives: {fn:5d}\")\n",
    "print(f\"  True Positives:  {tp:5d}\")\n",
    "\n",
    "print(f\"\\nMinority Class Metrics:\")\n",
    "print(f\"  Precision: {tp / (tp + fp):.4f} ({tp} / {tp + fp})\")\n",
    "print(f\"  Recall:    {tp / (tp + fn):.4f} ({tp} / {tp + fn})\")\n",
    "print(f\"  F1 Score:  {2*tp / (2*tp + fp + fn):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: BEST PRACTICES FOR IMBALANCED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = f\"\"\"\n",
    "âœ… Dataset:\n",
    "   â€¢ Imbalance ratio: {imbalance_ratio:.1f}:1\n",
    "   â€¢ Prevalence: {prevalence:.2%}\n",
    "\n",
    "âœ… Data Splitting:\n",
    "   â€¢ Used stratified train-test split\n",
    "   â€¢ Maintains class distribution in both sets\n",
    "\n",
    "âœ… Cross-Validation:\n",
    "   â€¢ StratifiedKFold (5 folds)\n",
    "   â€¢ Fair evaluation across all folds\n",
    "\n",
    "âœ… Pipeline (Prevents Data Leakage):\n",
    "   â€¢ Target encoding INSIDE pipeline\n",
    "   â€¢ Fitted separately on each CV fold\n",
    "   â€¢ No leakage from validation data\n",
    "\n",
    "âœ… Class Imbalance Handling:\n",
    "   â€¢ class_weight='balanced' in RandomForest\n",
    "   â€¢ Adjusts loss function to focus on minority class\n",
    "\n",
    "âœ… Calibration:\n",
    "   â€¢ Applied Platt scaling (sigmoid method)\n",
    "   â€¢ Brier score improved: {brier_uncal:.4f} â†’ {brier_cal:.4f}\n",
    "   â€¢ Enables reliable threshold selection\n",
    "\n",
    "âœ… Evaluation Metrics:\n",
    "   â€¢ ROC-AUC: {test_roc_auc:.4f} (discrimination ability)\n",
    "   â€¢ PR-AUC:  {test_pr_auc:.4f} ({test_pr_auc/test_prevalence:.1f}x better than random)\n",
    "   â€¢ Prevalence baseline: {test_prevalence:.4f}\n",
    "\n",
    "âœ… Threshold Selection:\n",
    "   â€¢ Explored 4 methods\n",
    "   â€¢ Chose F1-optimal: {final_threshold:.3f}\n",
    "   â€¢ Default 0.5 threshold is often suboptimal!\n",
    "\n",
    "âœ… Final Performance (threshold={final_threshold:.3f}):\n",
    "   â€¢ Precision: {tp / (tp + fp):.4f}\n",
    "   â€¢ Recall:    {tp / (tp + fn):.4f}\n",
    "   â€¢ F1 Score:  {2*tp / (2*tp + fp + fn):.4f}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WORKFLOW COMPLETE! ðŸŽ‰\")\n",
    "print(\"=\"*70)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deep Dive: Threshold Selection Methods\n",
    "\n",
    "Let me explain each threshold selection method in detail:\n",
    "\n",
    "### Method 1: Maximize F1 Score\n",
    "\n",
    "**When to use:** Balanced importance of precision and recall\n",
    "\n",
    "```python\n",
    "def find_optimal_threshold_f1(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Find threshold that maximizes F1 score\n",
    "    \"\"\"\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_scores = 2 * (precision_vals[:-1] * recall_vals[:-1]) / \\\n",
    "                (precision_vals[:-1] + recall_vals[:-1] + 1e-10)\n",
    "    \n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, f1_scores[optimal_idx]\n",
    "\n",
    "threshold, max_f1 = find_optimal_threshold_f1(y_test, y_proba_cal)\n",
    "print(f\"Optimal threshold: {threshold:.3f}, Max F1: {max_f1:.4f}\")\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Balances precision and recall\n",
    "- âœ… Single number to optimize\n",
    "- âœ… Works well for general use cases\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Assumes equal importance of precision and recall\n",
    "- âŒ May not suit business constraints\n",
    "\n",
    "---\n",
    "\n",
    "### Method 2: Youden's J Statistic\n",
    "\n",
    "**When to use:** Want to maximize true positive rate while minimizing false positive rate\n",
    "\n",
    "```python\n",
    "def find_optimal_threshold_youden(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Find threshold that maximizes Youden's J = TPR - FPR\n",
    "    Also called \"sensitivity + specificity - 1\"\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    \n",
    "    # J = TPR - FPR = Sensitivity + Specificity - 1\n",
    "    j_scores = tpr - fpr\n",
    "    \n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, j_scores[optimal_idx], tpr[optimal_idx], fpr[optimal_idx]\n",
    "\n",
    "threshold, j_score, tpr, fpr = find_optimal_threshold_youden(y_test, y_proba_cal)\n",
    "print(f\"Optimal threshold: {threshold:.3f}\")\n",
    "print(f\"J statistic: {j_score:.4f}\")\n",
    "print(f\"TPR: {tpr:.4f}, FPR: {fpr:.4f}\")\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **J = 0**: No better than random\n",
    "- **J = 1**: Perfect classifier\n",
    "- **J = 0.5**: Good practical performance\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Based on ROC curve (well-understood)\n",
    "- âœ… Maximizes overall classification performance\n",
    "- âœ… Accounts for both sensitivity and specificity\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Assumes equal importance of TPR and FPR\n",
    "- âŒ May not reflect business costs\n",
    "\n",
    "---\n",
    "\n",
    "### Method 3: Target Specific Recall\n",
    "\n",
    "**When to use:** Business requires minimum recall (e.g., must catch 80% of fraud)\n",
    "\n",
    "```python\n",
    "def find_threshold_for_target_recall(y_true, y_proba, target_recall=0.80):\n",
    "    \"\"\"\n",
    "    Find threshold that achieves target recall\n",
    "    while maximizing precision\n",
    "    \"\"\"\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # Find thresholds where recall >= target\n",
    "    valid_indices = np.where(recall_vals[:-1] >= target_recall)[0]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        print(f\"Warning: Cannot achieve {target_recall:.0%} recall\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Among valid thresholds, pick one with highest precision\n",
    "    optimal_idx = valid_indices[np.argmax(precision_vals[:-1][valid_indices])]\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return (optimal_threshold, \n",
    "            precision_vals[optimal_idx], \n",
    "            recall_vals[optimal_idx])\n",
    "\n",
    "threshold, prec, rec = find_threshold_for_target_recall(\n",
    "    y_test, y_proba_cal, target_recall=0.80\n",
    ")\n",
    "\n",
    "print(f\"Threshold for 80% recall: {threshold:.3f}\")\n",
    "print(f\"Achieved - Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "- **Fraud detection**: Must catch â‰¥90% of fraud (high recall)\n",
    "- **Disease screening**: Must catch â‰¥95% of cases (high recall)\n",
    "- **Spam filtering**: Must not miss â‰¥80% of spam (high recall)\n",
    "\n",
    "**Similarly, you can target precision:**\n",
    "\n",
    "```python\n",
    "def find_threshold_for_target_precision(y_true, y_proba, target_precision=0.90):\n",
    "    \"\"\"\n",
    "    Find threshold that achieves target precision\n",
    "    while maximizing recall\n",
    "    \"\"\"\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # Find thresholds where precision >= target\n",
    "    valid_indices = np.where(precision_vals[:-1] >= target_precision)[0]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        print(f\"Warning: Cannot achieve {target_precision:.0%} precision\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Among valid thresholds, pick one with highest recall\n",
    "    optimal_idx = valid_indices[np.argmax(recall_vals[:-1][valid_indices])]\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return (optimal_threshold, \n",
    "            precision_vals[optimal_idx], \n",
    "            recall_vals[optimal_idx])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Method 4: Cost-Sensitive Threshold\n",
    "\n",
    "**When to use:** Different costs for false positives vs false negatives\n",
    "\n",
    "```python\n",
    "def find_optimal_threshold_cost(y_true, y_proba, cost_fp=1, cost_fn=10):\n",
    "    \"\"\"\n",
    "    Find threshold that minimizes total cost\n",
    "    \n",
    "    Cost = (FP Ã— cost_fp) + (FN Ã— cost_fn)\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0.01, 0.99, 200)\n",
    "    costs = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        total_cost = fp * cost_fp + fn * cost_fn\n",
    "        costs.append(total_cost)\n",
    "    \n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    min_cost = costs[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, min_cost, costs, thresholds\n",
    "\n",
    "# Example: Missing fraud (FN) costs 10x more than false alarm (FP)\n",
    "threshold, min_cost, costs, thresholds = find_optimal_threshold_cost(\n",
    "    y_test, y_proba_cal, cost_fp=1, cost_fn=10\n",
    ")\n",
    "\n",
    "print(f\"Optimal threshold: {threshold:.3f}\")\n",
    "print(f\"Minimum cost: {min_cost:.0f}\")\n",
    "\n",
    "# Plot cost vs threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, costs, linewidth=2)\n",
    "plt.axvline(threshold, color='red', linestyle='--', \n",
    "            label=f'Optimal = {threshold:.3f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Total Cost')\n",
    "plt.title(f'Cost-Sensitive Threshold (FP cost={1}, FN cost={10})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Real-world cost examples:**\n",
    "\n",
    "```python\n",
    "# Fraud detection\n",
    "# FN (miss fraud): Lose $500 average transaction\n",
    "# FP (false alarm): $10 investigation cost\n",
    "cost_fp = 10\n",
    "cost_fn = 500\n",
    "\n",
    "# Medical diagnosis\n",
    "# FN (miss disease): Patient dies, $1,000,000 lawsuit\n",
    "# FP (false positive): $1,000 additional tests\n",
    "cost_fp = 1000\n",
    "cost_fn = 1000000\n",
    "\n",
    "# Email spam\n",
    "# FN (miss spam): 1 minute wasted\n",
    "# FP (good email marked spam): 10 minutes to recover\n",
    "cost_fp = 10\n",
    "cost_fn = 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Method 5: F-Beta Score\n",
    "\n",
    "**When to use:** Want to weight recall Î² times more than precision\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def find_optimal_threshold_fbeta(y_true, y_proba, beta=2.0):\n",
    "    \"\"\"\n",
    "    Find threshold that maximizes F-beta score\n",
    "    \n",
    "    beta > 1: Favors recall\n",
    "    beta < 1: Favors precision\n",
    "    beta = 1: F1 score (balanced)\n",
    "    \"\"\"\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # F-beta = (1 + betaÂ²) * (precision * recall) / (betaÂ² * precision + recall)\n",
    "    fbeta_scores = (1 + beta**2) * (precision_vals[:-1] * recall_vals[:-1]) / \\\n",
    "                   (beta**2 * precision_vals[:-1] + recall_vals[:-1] + 1e-10)\n",
    "    \n",
    "    optimal_idx = np.argmax(fbeta_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, fbeta_scores[optimal_idx]\n",
    "\n",
    "# F2 score: Recall is 2x more important than precision\n",
    "threshold_f2, max_f2 = find_optimal_threshold_fbeta(y_test, y_proba_cal, beta=2.0)\n",
    "print(f\"F2-optimal threshold: {threshold_f2:.3f}, Max F2: {max_f2:.4f}\")\n",
    "\n",
    "# F0.5 score: Precision is 2x more important than recall\n",
    "threshold_f05, max_f05 = find_optimal_threshold_fbeta(y_test, y_proba_cal, beta=0.5)\n",
    "print(f\"F0.5-optimal threshold: {threshold_f05:.3f}, Max F0.5: {max_f05:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Framework for Threshold Selection\n",
    "\n",
    "```\n",
    "START: Which threshold method?\n",
    "â”‚\n",
    "â”œâ”€ Equal importance of precision & recall?\n",
    "â”‚   YES â†’ Use F1 Score (Method 1) âœ…\n",
    "â”‚\n",
    "â”œâ”€ Business requires minimum recall (e.g., 80%)?\n",
    "â”‚   YES â†’ Use Target Recall (Method 3) âœ…\n",
    "â”‚\n",
    "â”œâ”€ Business requires minimum precision (e.g., 90%)?\n",
    "â”‚   YES â†’ Use Target Precision (Method 3 variant) âœ…\n",
    "â”‚\n",
    "â”œâ”€ Know exact costs of FP and FN?\n",
    "â”‚   YES â†’ Use Cost-Sensitive (Method 4) âœ…\n",
    "â”‚\n",
    "â”œâ”€ Recall more important than precision?\n",
    "â”‚   YES â†’ Use F-beta with beta > 1 (Method 5) âœ…\n",
    "â”‚\n",
    "â”œâ”€ Precision more important than recall?\n",
    "â”‚   YES â†’ Use F-beta with beta < 1 (Method 5) âœ…\n",
    "â”‚\n",
    "â””â”€ No specific requirements?\n",
    "    â†’ Use Youden's J (Method 2) âœ…\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Tips\n",
    "\n",
    "1. **Always calibrate before threshold tuning**\n",
    "   - Uncalibrated probabilities â†’ Wrong thresholds\n",
    "   - Calibrate first, then find threshold\n",
    "\n",
    "2. **Validate on separate dataset**\n",
    "   - Find threshold on validation set\n",
    "   - Test final threshold on test set\n",
    "   - Prevents overfitting to threshold\n",
    "\n",
    "3. **Plot threshold vs metrics**\n",
    "   - Visualize trade-offs\n",
    "   - Understand sensitivity to threshold changes\n",
    "\n",
    "4. **Document your choice**\n",
    "   - Why this threshold?\n",
    "   - What trade-offs were made?\n",
    "   - Business justification\n",
    "\n",
    "5. **Monitor in production**\n",
    "   - Threshold may need adjustment\n",
    "   - Data distribution changes over time\n",
    "\n",
    "Want me to show:\n",
    "1. **How to implement threshold selection in production** (save/load thresholds)?\n",
    "2. **Advanced: Multi-class threshold optimization**?\n",
    "3. **How to handle threshold drift over time**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ed982",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
